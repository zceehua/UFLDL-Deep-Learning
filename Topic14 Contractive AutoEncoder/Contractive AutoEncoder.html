<!DOCTYPE html>
<!-- saved from url=(0049)http://www.cnblogs.com/tornadomeet/p/3434651.html -->
<html lang="zh-cn"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Deep learning：四十八(Contractive AutoEncoder简单理解) - tornadomeet - 博客园</title>
<link type="text/css" rel="stylesheet" href="./Contractive AutoEncoder_files/blog-common.css">
<link id="MainCss" type="text/css" rel="stylesheet" href="./Contractive AutoEncoder_files/bundle-sea.css">
<link id="mobile-style" media="only screen and (max-width: 768px)" type="text/css" rel="stylesheet" href="./Contractive AutoEncoder_files/bundle-sea-mobile.css">
<link title="RSS" type="application/rss+xml" rel="alternate" href="http://www.cnblogs.com/tornadomeet/rss">
<link title="RSD" type="application/rsd+xml" rel="EditURI" href="http://www.cnblogs.com/tornadomeet/rsd.xml">
<link type="application/wlwmanifest+xml" rel="wlwmanifest" href="http://www.cnblogs.com/tornadomeet/wlwmanifest.xml">
<script type="text/javascript" src="./Contractive AutoEncoder_files/encoder.js.下载"></script><script src="./Contractive AutoEncoder_files/jquery.js.下载" type="text/javascript"></script>  
<script type="text/javascript">var currentBlogApp = 'tornadomeet', cb_enable_mathjax=false;var isLogined=false;</script>
<script src="./Contractive AutoEncoder_files/blog-common.js.下载" type="text/javascript"></script>
</head>
<body>
<a name="top"></a>

<!--done-->
<div id="header">
	
<!--done-->
<div class="header">
	<div class="headerText">
		<a id="Header1_HeaderTitle" class="headermaintitle" href="http://www.cnblogs.com/tornadomeet/">tornadomeet</a><br>
		
	</div>
</div>

</div>

<div id="mytopmenu">
	
		<div id="mylinks"><a id="blog_nav_sitehome" class="menu" href="http://www.cnblogs.com/">博客园</a> &nbsp;
<a id="blog_nav_myhome" class="menu" href="http://www.cnblogs.com/tornadomeet/">首页</a> &nbsp;
<a id="blog_nav_newpost" class="menu" rel="nofollow" href="https://i.cnblogs.com/EditPosts.aspx?opt=1">新随笔</a> &nbsp;
<a id="blog_nav_contact" class="menu" rel="nofollow" href="https://msg.cnblogs.com/send/tornadomeet">联系</a> &nbsp;
<a id="blog_nav_rss" class="menu" href="http://www.cnblogs.com/tornadomeet/rss">订阅</a><a id="blog_nav_rss_image" href="http://www.cnblogs.com/tornadomeet/rss"><img src="./Contractive AutoEncoder_files/xml.gif" alt="订阅"></a>&nbsp;
<a id="blog_nav_admin" class="menu" rel="nofollow" href="https://i.cnblogs.com/">管理</a>
</div>
		<div id="mystats"><div id="blog_stats">
随笔-252&nbsp;
评论-2165&nbsp;
文章-0&nbsp;
<!--trackbacks-0-->
</div></div>
	
</div>
<div id="centercontent">
	
<div id="post_detail">
<div class="post">
	<h1 class="postTitle"><a id="cb_post_title_url" class="postTitle2" href="http://www.cnblogs.com/tornadomeet/p/3434651.html">Deep learning：四十八(Contractive AutoEncoder简单理解)</a></h1>
	<div id="cnblogs_post_body"><div class="Section1">
<p>&nbsp;</p>
<p>　　Contractive autoencoder是autoencoder的一个变种，其实就是在autoencoder上加入了一个规则项，它简称CAE（对应中文翻译为？）。通常情况下，对权值进行惩罚后的autoencoder数学表达形式为：</p>
<p>&nbsp;　　<img src="./Contractive AutoEncoder_files/20232936-c9cce089e03a40cfbdef6f1fb49592e8.png" alt="" width="411" height="75"></p>
<p>　　这是直接对W的值进行惩罚的，而今天要讲的CAE其数学表达式同样非常简单，如下：</p>
<p>&nbsp;　　<img src="./Contractive AutoEncoder_files/20232954-09b375c1f96c4b7d96570991eeccce99.png" alt=""></p>
<p>　　其中的<img src="./Contractive AutoEncoder_files/20233018-9aea135bf9d14c5b87875351a8828b9c.png" alt="" width="42" height="23">&nbsp;是隐含层输出值关于权重的雅克比矩阵，而&nbsp;<img src="./Contractive AutoEncoder_files/20233041-43b0afda2b124129b8bd05aac8519edc.png" alt="" width="72" height="32">&nbsp; 表示的是该雅克比矩阵的F范数的平方，即雅克比矩阵中每个元素求平方</p>
<p>　　<img src="./Contractive AutoEncoder_files/20233253-67453f12fc7d4e85a08d563fae0b1714.png" alt="" width="226" height="64"></p>
<p>　　然后求和，更具体的数学表达式为：</p>
<p>　　<img src="./Contractive AutoEncoder_files/20233119-5504b0dd531742c6801ceb791baa5c04.png" alt="" width="278" height="66">&nbsp;</p>
<p>　　关于雅克比矩阵的介绍可参考<a href="http://blog.csdn.net/carrierlxksuper/article/details/12453307">雅克比矩阵&amp;行列式——单纯的矩阵和算子</a>，关于F范数可参考我前面的博文<a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/14/3019885.html">Sparse coding中关于矩阵的范数求导</a>中的内容。</p>
<p>　　有了loss函数的表达式，采用常见的mini-batch随机梯度下降法训练即可。</p>
<p>　　关于为什么contrative autoencoder效果这么好？paper中作者解释了好几页，好吧，我真没完全明白，希望懂的朋友能简单通俗的介绍下。下面是读完文章中的一些理解：</p>
<p>　　好的特征表示大致有2个衡量标准：1. 可以很好的重构出输入数据; 2.对输入数据一定程度下的扰动具有不变形。普通的autoencoder和sparse autoencoder主要是符合第一个标准。而deniose autoencoder和contractive autoencoder则主要体现在第二个。而作为分类任务来说，第二个标准显得更重要。</p>
<p>　　雅克比矩阵包含数据在各种方向上的信息，可以对雅克比矩阵进行奇异值分解，同时画出奇异值数目和奇异值的曲线图，大的奇异值对应着学习到的局部方向可允许的变化量，并且曲线越抖越好（这个图没看明白，所以这里的解释基本上是直接翻译原文中某些观点）。</p>
<p>　　另一个曲线图是contractive ratio图，contractive ratio定义为：原空间中2个样本直接的距离比上特征空间（指映射后的空间）中对应2个样本点之间的距离。某个点x处局部映射的contraction值是指该点处雅克比矩阵的F范数。按照作者的观点，contractive ration曲线呈上升趋势的话更好（why？），而CAE刚好符合。</p>
<p>　　总之Contractive autoencoder主要是抑制训练样本（处在低维流形曲面上）在所有方向上的扰动。</p>
<p>　　CAE的代码可参考：<a href="file:///https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/cA.py">pylearn2/cA.py</a>&nbsp;　　</p>
<div class="cnblogs_code"><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a href="javascript:void(0);" onclick="copyCnblogsCode(this)" title="复制代码"><img src="./Contractive AutoEncoder_files/copycode.gif" alt="复制代码"></a></span></div>
<pre><span style="color: #800000;">"""</span><span style="color: #800000;">This tutorial introduces Contractive auto-encoders (cA) using Theano.

 They are based on auto-encoders as the ones used in Bengio et
 al. 2007.  An autoencoder takes an input x and first maps it to a
 hidden representation y = f_{\theta}(x) = s(Wx+b), parameterized by
 \theta={W,b}. The resulting latent representation y is then mapped
 back to a "reconstructed" vector z \in [0,1]^d in input space z =
 g_{\theta'}(y) = s(W'y + b').  The weight matrix W' can optionally be
 constrained such that W' = W^T, in which case the autoencoder is said
 to have tied weights. The network is trained such that to minimize
 the reconstruction error (the error between x and z).  Adding the
 squared Frobenius norm of the Jacobian of the hidden mapping h with
 respect to the visible units yields the contractive auto-encoder:

      - \sum_{k=1}^d[ x_k \log z_k + (1-x_k) \log( 1-z_k)]  + \| \frac{\partial h(x)}{\partial x} \|^2

 References :
   - S. Rifai, P. Vincent, X. Muller, X. Glorot, Y. Bengio: Contractive
   Auto-Encoders: Explicit Invariance During Feature Extraction, ICML-11

   - S. Rifai, X. Muller, X. Glorot, G. Mesnil, Y. Bengio, and Pascal
     Vincent. Learning invariant features through local space
     contraction. Technical Report 1360, Universite de Montreal

   - Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle: Greedy Layer-Wise
   Training of Deep Networks, Advances in Neural Information Processing
   Systems 19, 2007

</span><span style="color: #800000;">"""</span>
<span style="color: #0000ff;">import</span><span style="color: #000000;"> cPickle
</span><span style="color: #0000ff;">import</span><span style="color: #000000;"> gzip
</span><span style="color: #0000ff;">import</span><span style="color: #000000;"> os
</span><span style="color: #0000ff;">import</span><span style="color: #000000;"> sys
</span><span style="color: #0000ff;">import</span><span style="color: #000000;"> time

</span><span style="color: #0000ff;">import</span><span style="color: #000000;"> numpy

</span><span style="color: #0000ff;">import</span><span style="color: #000000;"> theano
</span><span style="color: #0000ff;">import</span><span style="color: #000000;"> theano.tensor as T


</span><span style="color: #0000ff;">from</span> logistic_sgd <span style="color: #0000ff;">import</span><span style="color: #000000;"> load_data
</span><span style="color: #0000ff;">from</span> utils <span style="color: #0000ff;">import</span><span style="color: #000000;"> tile_raster_images

</span><span style="color: #0000ff;">import</span><span style="color: #000000;"> PIL.Image


</span><span style="color: #0000ff;">class</span><span style="color: #000000;"> cA(object):
    </span><span style="color: #800000;">"""</span><span style="color: #800000;"> Contractive Auto-Encoder class (cA)

    The contractive autoencoder tries to reconstruct the input with an
    additional constraint on the latent space. With the objective of
    obtaining a robust representation of the input space, we
    regularize the L2 norm(Froebenius) of the jacobian of the hidden
    representation with respect to the input. Please refer to Rifai et
    al.,2011 for more details.

    If x is the input then equation (1) computes the projection of the
    input into the latent space h. Equation (2) computes the jacobian
    of h with respect to x.  Equation (3) computes the reconstruction
    of the input, while equation (4) computes the reconstruction
    error and the added regularization term from Eq.(2).

    .. math::

        h_i = s(W_i x + b_i)                                             (1)

        J_i = h_i (1 - h_i) * W_i                                        (2)

        x' = s(W' h  + b')                                               (3)

        L = -sum_{k=1}^d [x_k \log x'_k + (1-x_k) \log( 1-x'_k)]
             + lambda * sum_{i=1}^d sum_{j=1}^n J_{ij}^2                 (4)

    </span><span style="color: #800000;">"""</span>

    <span style="color: #0000ff;">def</span> <span style="color: #800080;">__init__</span>(self, numpy_rng, input=None, n_visible=784, n_hidden=100<span style="color: #000000;">,
                 n_batchsize</span>=1, W=None, bhid=None, bvis=<span style="color: #000000;">None):
        </span><span style="color: #800000;">"""</span><span style="color: #800000;">Initialize the cA class by specifying the number of visible units (the
        dimension d of the input ), the number of hidden units ( the dimension
        d' of the latent or hidden space ) and the contraction level. The
        constructor also receives symbolic variables for the input, weights and
        bias.

        :type numpy_rng: numpy.random.RandomState
        :param numpy_rng: number random generator used to generate weights

        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams
        :param theano_rng: Theano random generator; if None is given
                     one is generated based on a seed drawn from `rng`

        :type input: theano.tensor.TensorType
        :param input: a symbolic description of the input or None for
                      standalone cA

        :type n_visible: int
        :param n_visible: number of visible units

        :type n_hidden: int
        :param n_hidden:  number of hidden units

        :type n_batchsize int
        :param n_batchsize: number of examples per batch

        :type W: theano.tensor.TensorType
        :param W: Theano variable pointing to a set of weights that should be
                  shared belong the dA and another architecture; if dA should
                  be standalone set this to None

        :type bhid: theano.tensor.TensorType
        :param bhid: Theano variable pointing to a set of biases values (for
                     hidden units) that should be shared belong dA and another
                     architecture; if dA should be standalone set this to None

        :type bvis: theano.tensor.TensorType
        :param bvis: Theano variable pointing to a set of biases values (for
                     visible units) that should be shared belong dA and another
                     architecture; if dA should be standalone set this to None

        </span><span style="color: #800000;">"""</span><span style="color: #000000;">
        self.n_visible </span>=<span style="color: #000000;"> n_visible
        self.n_hidden </span>=<span style="color: #000000;"> n_hidden
        self.n_batchsize </span>=<span style="color: #000000;"> n_batchsize
        </span><span style="color: #008000;">#</span><span style="color: #008000;"> note : W' was written as `W_prime` and b' as `b_prime`</span>
        <span style="color: #0000ff;">if</span> <span style="color: #0000ff;">not</span><span style="color: #000000;"> W:
            </span><span style="color: #008000;">#</span><span style="color: #008000;"> W is initialized with `initial_W` which is uniformely sampled</span>
            <span style="color: #008000;">#</span><span style="color: #008000;"> from -4*sqrt(6./(n_visible+n_hidden)) and</span>
            <span style="color: #008000;">#</span><span style="color: #008000;"> 4*sqrt(6./(n_hidden+n_visible))the output of uniform if</span>
            <span style="color: #008000;">#</span><span style="color: #008000;"> converted using asarray to dtype</span>
            <span style="color: #008000;">#</span><span style="color: #008000;"> theano.config.floatX so that the code is runable on GPU</span>
            initial_W =<span style="color: #000000;"> numpy.asarray(numpy_rng.uniform(
                      low</span>=-4 * numpy.sqrt(6. / (n_hidden +<span style="color: #000000;"> n_visible)),
                      high</span>=4 * numpy.sqrt(6. / (n_hidden +<span style="color: #000000;"> n_visible)),
                      size</span>=<span style="color: #000000;">(n_visible, n_hidden)),
                                      dtype</span>=<span style="color: #000000;">theano.config.floatX)
            W </span>= theano.shared(value=initial_W, name=<span style="color: #800000;">'</span><span style="color: #800000;">W</span><span style="color: #800000;">'</span>, borrow=<span style="color: #000000;">True)

        </span><span style="color: #0000ff;">if</span> <span style="color: #0000ff;">not</span><span style="color: #000000;"> bvis:
            bvis </span>= theano.shared(value=<span style="color: #000000;">numpy.zeros(n_visible,
                                                   dtype</span>=<span style="color: #000000;">theano.config.floatX),
                                 borrow</span>=<span style="color: #000000;">True)

        </span><span style="color: #0000ff;">if</span> <span style="color: #0000ff;">not</span><span style="color: #000000;"> bhid:
            bhid </span>= theano.shared(value=<span style="color: #000000;">numpy.zeros(n_hidden,
                                                   dtype</span>=<span style="color: #000000;">theano.config.floatX),
                                 name</span>=<span style="color: #800000;">'</span><span style="color: #800000;">b</span><span style="color: #800000;">'</span><span style="color: #000000;">,
                                 borrow</span>=<span style="color: #000000;">True)

        self.W </span>=<span style="color: #000000;"> W
        </span><span style="color: #008000;">#</span><span style="color: #008000;"> b corresponds to the bias of the hidden</span>
        self.b =<span style="color: #000000;"> bhid
        </span><span style="color: #008000;">#</span><span style="color: #008000;"> b_prime corresponds to the bias of the visible</span>
        self.b_prime =<span style="color: #000000;"> bvis
        </span><span style="color: #008000;">#</span><span style="color: #008000;"> tied weights, therefore W_prime is W transpose</span>
        self.W_prime =<span style="color: #000000;"> self.W.T

        </span><span style="color: #008000;">#</span><span style="color: #008000;"> if no input is given, generate a variable representing the input</span>
        <span style="color: #0000ff;">if</span> input ==<span style="color: #000000;"> None:
            </span><span style="color: #008000;">#</span><span style="color: #008000;"> we use a matrix because we expect a minibatch of several</span>
            <span style="color: #008000;">#</span><span style="color: #008000;"> examples, each example being a row</span>
            self.x = T.dmatrix(name=<span style="color: #800000;">'</span><span style="color: #800000;">input</span><span style="color: #800000;">'</span><span style="color: #000000;">)
        </span><span style="color: #0000ff;">else</span><span style="color: #000000;">:
            self.x </span>=<span style="color: #000000;"> input

        self.params </span>=<span style="color: #000000;"> [self.W, self.b, self.b_prime]

    </span><span style="color: #0000ff;">def</span> get_hidden_values(self, input): <span style="color: #008000;">#</span><span style="color: #008000;">激发函数为sigmoid看，这里只向前进一次</span>
        <span style="color: #800000;">"""</span><span style="color: #800000;"> Computes the values of the hidden layer </span><span style="color: #800000;">"""</span>
        <span style="color: #0000ff;">return</span> T.nnet.sigmoid(T.dot(input, self.W) +<span style="color: #000000;"> self.b)

    </span><span style="color: #0000ff;">def</span><span style="color: #000000;"> get_jacobian(self, hidden, W):
        </span><span style="color: #800000;">"""</span><span style="color: #800000;">Computes the jacobian of the hidden layer with respect to
        the input, reshapes are necessary for broadcasting the
        element-wise product on the right axis

        </span><span style="color: #800000;">"""</span>
        <span style="color: #0000ff;">return</span> T.reshape(hidden * (1 - hidden), <span style="color: #008000;">#</span><span style="color: #008000;">计算雅克比矩阵，先将h(1-h)变成3维矩阵，然后将w也变成3维矩阵，然后将这2个3维矩阵</span>
                         (self.n_batchsize, 1, self.n_hidden)) * T.reshape( <span style="color: #008000;">#</span><span style="color: #008000;">对应元素相乘，但怎么感觉2个矩阵尺寸不对应呢？</span>
                             W, (1<span style="color: #000000;">, self.n_visible, self.n_hidden))

    </span><span style="color: #0000ff;">def</span> get_reconstructed_input(self, hidden): <span style="color: #008000;">#</span><span style="color: #008000;">重构输入时获得的输出端数据</span>
        <span style="color: #800000;">"""</span><span style="color: #800000;">Computes the reconstructed input given the values of the
        hidden layer

        </span><span style="color: #800000;">"""</span>
        <span style="color: #0000ff;">return</span>  T.nnet.sigmoid(T.dot(hidden, self.W_prime) +<span style="color: #000000;"> self.b_prime)

    </span><span style="color: #0000ff;">def</span><span style="color: #000000;"> get_cost_updates(self, contraction_level, learning_rate):
        </span><span style="color: #800000;">"""</span><span style="color: #800000;"> This function computes the cost and the updates for one trainng
        step of the cA </span><span style="color: #800000;">"""</span><span style="color: #000000;">

        y </span>=<span style="color: #000000;"> self.get_hidden_values(self.x)
        z </span>=<span style="color: #000000;"> self.get_reconstructed_input(y)
        J </span>=<span style="color: #000000;"> self.get_jacobian(y, self.W)
        </span><span style="color: #008000;">#</span><span style="color: #008000;"> note : we sum over the size of a datapoint; if we are using</span>
        <span style="color: #008000;">#</span><span style="color: #008000;">        minibatches, L will be a vector, with one entry per</span>
        <span style="color: #008000;">#</span><span style="color: #008000;">        example in minibatch</span>
        self.L_rec = - T.sum(self.x * T.log(z) + <span style="color: #008000;">#</span><span style="color: #008000;">交叉熵作为重构误差(当输入是[0,1],且是sigmoid时可以采用)</span>
                             (1 - self.x) * T.log(1 -<span style="color: #000000;"> z),
                             axis</span>=1<span style="color: #000000;">)

        </span><span style="color: #008000;">#</span><span style="color: #008000;"> Compute the jacobian and average over the number of samples/minibatch</span>
        self.L_jacob = T.sum(J ** 2) /<span style="color: #000000;"> self.n_batchsize

        </span><span style="color: #008000;">#</span><span style="color: #008000;"> note : L is now a vector, where each element is the</span>
        <span style="color: #008000;">#</span><span style="color: #008000;">        cross-entropy cost of the reconstruction of the</span>
        <span style="color: #008000;">#</span><span style="color: #008000;">        corresponding example of the minibatch. We need to</span>
        <span style="color: #008000;">#</span><span style="color: #008000;">        compute the average of all these to get the cost of</span>
        <span style="color: #008000;">#</span><span style="color: #008000;">        the minibatch</span>
        cost = T.mean(self.L_rec) + contraction_level *<span style="color: #000000;"> T.mean(self.L_jacob)

        </span><span style="color: #008000;">#</span><span style="color: #008000;"> compute the gradients of the cost of the `cA` with respect</span>
        <span style="color: #008000;">#</span><span style="color: #008000;"> to its parameters</span>
        gparams = T.grad(cost, self.params) <span style="color: #008000;">#</span><span style="color: #008000;">Theano特有的功能，自动求导</span>
        <span style="color: #008000;">#</span><span style="color: #008000;"> generate the list of updates</span>
        updates =<span style="color: #000000;"> []
        </span><span style="color: #0000ff;">for</span> param, gparam <span style="color: #0000ff;">in</span><span style="color: #000000;"> zip(self.params, gparams):
            updates.append((param, param </span>- learning_rate * gparam)) <span style="color: #008000;">#</span><span style="color: #008000;">SGD算法 </span>

        <span style="color: #0000ff;">return</span><span style="color: #000000;"> (cost, updates)


</span><span style="color: #0000ff;">def</span> test_cA(learning_rate=0.01, training_epochs=20<span style="color: #000000;">,
            dataset</span>=<span style="color: #800000;">'</span><span style="color: #800000;">./data/mnist.pkl.gz</span><span style="color: #800000;">'</span><span style="color: #000000;">,
            batch_size</span>=10, output_folder=<span style="color: #800000;">'</span><span style="color: #800000;">cA_plots</span><span style="color: #800000;">'</span>, contraction_level=.1<span style="color: #000000;">):
    </span><span style="color: #800000;">"""</span><span style="color: #800000;">
    This demo is tested on MNIST

    :type learning_rate: float
    :param learning_rate: learning rate used for training the contracting
                          AutoEncoder

    :type training_epochs: int
    :param training_epochs: number of epochs used for training

    :type dataset: string
    :param dataset: path to the picked dataset

    </span><span style="color: #800000;">"""</span><span style="color: #000000;">
    datasets </span>=<span style="color: #000000;"> load_data(dataset)
    train_set_x, train_set_y </span>=<span style="color: #000000;"> datasets[0]

    </span><span style="color: #008000;">#</span><span style="color: #008000;"> compute number of minibatches for training, validation and testing</span>
    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size <span style="color: #008000;">#</span><span style="color: #008000;">标识borrow=True表示不需要复制样本</span>

    <span style="color: #008000;">#</span><span style="color: #008000;"> allocate symbolic variables for the data</span>
    index = T.lscalar()    <span style="color: #008000;">#</span><span style="color: #008000;"> index to a [mini]batch</span>
    x = T.matrix(<span style="color: #800000;">'</span><span style="color: #800000;">x</span><span style="color: #800000;">'</span>)  <span style="color: #008000;">#</span><span style="color: #008000;"> the data is presented as rasterized images</span>

    <span style="color: #0000ff;">if</span> <span style="color: #0000ff;">not</span><span style="color: #000000;"> os.path.isdir(output_folder):
        os.makedirs(output_folder)
    os.chdir(output_folder)
    </span><span style="color: #008000;">#</span><span style="color: #008000;">###################################</span>
    <span style="color: #008000;">#</span><span style="color: #008000;">        BUILDING THE MODEL        #</span>
    <span style="color: #008000;">#</span><span style="color: #008000;">###################################</span>
<span style="color: #000000;">
    rng </span>= numpy.random.RandomState(123<span style="color: #000000;">)

    ca </span>= cA(numpy_rng=rng, input=<span style="color: #000000;">x,
            n_visible</span>=28 * 28, n_hidden=500, n_batchsize=batch_size) <span style="color: #008000;">#</span><span style="color: #008000;">500个隐含层节点</span>
<span style="color: #000000;">
    cost, updates </span>= ca.get_cost_updates(contraction_level=contraction_level, <span style="color: #008000;">#</span><span style="color: #008000;">update里面装的是参数的更新过程</span>
                                        learning_rate=<span style="color: #000000;">learning_rate)

    train_ca </span>= theano.function([index], [T.mean(ca.L_rec), ca.L_jacob], <span style="color: #008000;">#</span><span style="color: #008000;">定义函数，输入为batch的索引，输出为该batch下的重构误差和雅克比误差</span>
                               updates=<span style="color: #000000;">updates,
                               givens</span>={x: train_set_x[index *<span style="color: #000000;"> batch_size:
                                                    (index </span>+ 1) *<span style="color: #000000;"> batch_size]})

    start_time </span>=<span style="color: #000000;"> time.clock()

    </span><span style="color: #008000;">#</span><span style="color: #008000;">###########</span>
    <span style="color: #008000;">#</span><span style="color: #008000;"> TRAINING #</span>
    <span style="color: #008000;">#</span><span style="color: #008000;">###########</span>

    <span style="color: #008000;">#</span><span style="color: #008000;"> go through training epochs</span>
    <span style="color: #0000ff;">for</span> epoch <span style="color: #0000ff;">in</span> xrange(training_epochs): <span style="color: #008000;">#</span><span style="color: #008000;">循环20次</span>
        <span style="color: #008000;">#</span><span style="color: #008000;"> go through trainng set</span>
        c =<span style="color: #000000;"> []
        </span><span style="color: #0000ff;">for</span> batch_index <span style="color: #0000ff;">in</span><span style="color: #000000;"> xrange(n_train_batches):
            c.append(train_ca(batch_index)) </span><span style="color: #008000;">#</span><span style="color: #008000;">计算loss值,计算过程中其实也一直在更新updates权值</span>
<span style="color: #000000;">
        c_array </span>= numpy.vstack(c) <span style="color: #008000;">#</span><span style="color: #008000;">vstack()为将矩阵序列c按照每行叠加，重新构造一个矩阵</span>
        <span style="color: #0000ff;">print</span> <span style="color: #800000;">'</span><span style="color: #800000;">Training epoch %d, reconstruction cost </span><span style="color: #800000;">'</span> %<span style="color: #000000;"> epoch, numpy.mean(
            c_array[0]), </span><span style="color: #800000;">'</span><span style="color: #800000;"> jacobian norm </span><span style="color: #800000;">'</span>, numpy.mean(numpy.sqrt(c_array[1<span style="color: #000000;">]))

    end_time </span>=<span style="color: #000000;"> time.clock()

    training_time </span>= (end_time -<span style="color: #000000;"> start_time)
    </span><span style="color: #008000;">#</span><span style="color: #008000;">下面是显示和保存学习到的权值结果</span>
    <span style="color: #0000ff;">print</span> &gt;&gt; sys.stderr, (<span style="color: #800000;">'</span><span style="color: #800000;">The code for file </span><span style="color: #800000;">'</span> + os.path.split(<span style="color: #800080;">__file__</span>)[1] +
                          <span style="color: #800000;">'</span><span style="color: #800000;"> ran for %.2fm</span><span style="color: #800000;">'</span> % ((training_time) / 60<span style="color: #000000;">.))
    image </span>=<span style="color: #000000;"> PIL.Image.fromarray(tile_raster_images(
        X</span>=ca.W.get_value(borrow=<span style="color: #000000;">True).T,
        img_shape</span>=(28, 28), tile_shape=(10, 10<span style="color: #000000;">),
        tile_spacing</span>=(1, 1<span style="color: #000000;">)))

    image.save(</span><span style="color: #800000;">'</span><span style="color: #800000;">cae_filters.png</span><span style="color: #800000;">'</span><span style="color: #000000;">)

    os.chdir(</span><span style="color: #800000;">'</span><span style="color: #800000;">../</span><span style="color: #800000;">'</span><span style="color: #000000;">)


</span><span style="color: #0000ff;">if</span> <span style="color: #800080;">__name__</span> == <span style="color: #800000;">'</span><span style="color: #800000;">__main__</span><span style="color: #800000;">'</span><span style="color: #000000;">:
    test_cA()</span></pre>
<div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a href="javascript:void(0);" onclick="copyCnblogsCode(this)" title="复制代码"><img src="./Contractive AutoEncoder_files/copycode.gif" alt="复制代码"></a></span></div></div>
<p><span style="line-height: 1.5;">　　按照原程序，迭代20次，跑了6个多小时，重构误差项和contraction项变化情况如下：</span></p>
<div class="cnblogs_code"><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a href="javascript:void(0);" onclick="copyCnblogsCode(this)" title="复制代码"><img src="./Contractive AutoEncoder_files/copycode.gif" alt="复制代码"></a></span></div>
<pre><span style="color: #000000;">... loading data
Training epoch 0, reconstruction cost  </span>589.571872577  jacobian norm  20.9938791886<span style="color: #000000;">
Training epoch </span>1, reconstruction cost  115.13390224  jacobian norm  10.673699659<span style="color: #000000;">
Training epoch </span>2, reconstruction cost  101.291018001  jacobian norm  10.134422748<span style="color: #000000;">
Training epoch </span>3, reconstruction cost  94.220284334  jacobian norm  9.84685383242<span style="color: #000000;">
Training epoch </span>4, reconstruction cost  89.5890225412  jacobian norm  9.64736166807<span style="color: #000000;">
Training epoch </span>5, reconstruction cost  86.1490384385  jacobian norm  9.49857669084<span style="color: #000000;">
Training epoch </span>6, reconstruction cost  83.4664242016  jacobian norm  9.38143172793<span style="color: #000000;">
Training epoch </span>7, reconstruction cost  81.3512907826  jacobian norm  9.28327421556<span style="color: #000000;">
Training epoch </span>8, reconstruction cost  79.6482831506  jacobian norm  9.19748922967<span style="color: #000000;">
Training epoch </span>9, reconstruction cost  78.2066659332  jacobian norm  9.12143982155<span style="color: #000000;">
Training epoch </span>10, reconstruction cost  76.9456192804  jacobian norm  9.05343287129<span style="color: #000000;">
Training epoch </span>11, reconstruction cost  75.8435863545  jacobian norm  8.99151663486<span style="color: #000000;">
Training epoch </span>12, reconstruction cost  74.8999458491  jacobian norm  8.9338049163<span style="color: #000000;">
Training epoch </span>13, reconstruction cost  74.1060022563  jacobian norm  8.87925367541<span style="color: #000000;">
Training epoch </span>14, reconstruction cost  73.4415396294  jacobian norm  8.8291852146<span style="color: #000000;">
Training epoch </span>15, reconstruction cost  72.879630175  jacobian norm  8.78442892358<span style="color: #000000;">
Training epoch </span>16, reconstruction cost  72.3729563995  jacobian norm  8.74324402838<span style="color: #000000;">
Training epoch </span>17, reconstruction cost  71.8622392555  jacobian norm  8.70262903409<span style="color: #000000;">
Training epoch </span>18, reconstruction cost  71.3049790204  jacobian norm  8.66103980493<span style="color: #000000;">
Training epoch </span>19, reconstruction cost  70.6462751293  jacobian norm  8.61777944201</pre>
<div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a href="javascript:void(0);" onclick="copyCnblogsCode(this)" title="复制代码"><img src="./Contractive AutoEncoder_files/copycode.gif" alt="复制代码"></a></span></div></div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>　　<span style="font-size: 18pt;"><strong><span style="color: #0000ff;">参考资料：</span></strong></span></p>
<p>&nbsp; &nbsp; &nbsp;&nbsp;Contractive auto-encoders: Explicit invariance during feature extraction，Salah Rifai，Pascal Vincent，Xavier Muller，Xavier Glorot，Yoshua Bengio</p>
<p>　 &nbsp;<a href="http://blog.csdn.net/carrierlxksuper/article/details/12453307">雅克比矩阵&amp;行列式——单纯的矩阵和算子</a></p>
<p>　 &nbsp;<a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/14/3019885.html">Sparse coding中关于矩阵的范数求导</a></p>
<p>&nbsp; &nbsp; &nbsp;&nbsp;<a href="file:///https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/cA.py">pylearn2/cA.py</a></p>
<p>&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://blog.csdn.net/carrierlxksuper/article/details/12859383">再谈雅克比矩阵---在feature learning中的作用</a></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</div></div><div id="MySignature" style="display: block;">作者：tornadomeet

出处：http://www.cnblogs.com/tornadomeet

欢迎转载或分享，但请务必声明文章出处。      （新浪微博：tornadomeet,欢迎交流！）</div>
<div class="clear"></div>
<div id="blog_post_info_block">
<div id="BlogPostCategory">分类: <a href="http://www.cnblogs.com/tornadomeet/category/497607.html" target="_blank">Deep Learning</a>,<a href="http://www.cnblogs.com/tornadomeet/category/361811.html" target="_blank">机器学习</a></div>
<div id="EntryTag">标签: <a href="http://www.cnblogs.com/tornadomeet/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>, <a href="http://www.cnblogs.com/tornadomeet/tag/Deep%20Learning/">Deep Learning</a></div>
<div id="blog_post_info"><div id="green_channel">
        <a href="javascript:void(0);" id="green_channel_digg" onclick="DiggIt(3434651,cb_blogId,1);green_channel_success(this,&#39;谢谢推荐！&#39;);">好文要顶</a>
            <a id="green_channel_follow" onclick="follow(&#39;dae176a9-cc64-e111-aa3f-842b2b196315&#39;);" href="javascript:void(0);">关注我</a>
    <a id="green_channel_favorite" onclick="AddToWz(cb_entryId);return false;" href="javascript:void(0);">收藏该文</a>
    <a id="green_channel_weibo" href="javascript:void(0);" title="分享至新浪微博" onclick="ShareToTsina()"><img src="./Contractive AutoEncoder_files/icon_weibo_24.png" alt=""></a>
    <a id="green_channel_wechat" href="javascript:void(0);" title="分享至微信" onclick="shareOnWechat()"><img src="./Contractive AutoEncoder_files/wechat.png" alt=""></a>
</div>
<div id="author_profile">
    <div id="author_profile_info" class="author_profile_info">
            <a href="http://home.cnblogs.com/u/tornadomeet/" target="_blank"><img src="./Contractive AutoEncoder_files/sample_face.gif" class="author_avatar" alt=""></a>
        <div id="author_profile_detail" class="author_profile_info">
            <a href="http://home.cnblogs.com/u/tornadomeet/">tornadomeet</a><br>
            <a href="http://home.cnblogs.com/u/tornadomeet/followees">关注 - 46</a><br>
            <a href="http://home.cnblogs.com/u/tornadomeet/followers">粉丝 - 3284</a>
        </div>
    </div>
    <div class="clear"></div>
    <div id="author_profile_honor"></div>
    <div id="author_profile_follow">
                <a href="javascript:void(0);" onclick="follow(&#39;dae176a9-cc64-e111-aa3f-842b2b196315&#39;);return false;">+加关注</a>
    </div>
</div>
<div id="div_digg">
    <div class="diggit" onclick="votePost(3434651,&#39;Digg&#39;)">
        <span class="diggnum" id="digg_count">1</span>
    </div>
    <div class="buryit" onclick="votePost(3434651,&#39;Bury&#39;)">
        <span class="burynum" id="bury_count">0</span>
    </div>
    <div class="clear"></div>
    <div class="diggword" id="digg_tips">
    </div>
</div>
</div>
<div class="clear"></div>
<div id="post_next_prev"><a href="http://www.cnblogs.com/tornadomeet/p/3432093.html" class="p_n_p_prefix">« </a> 上一篇：<a href="http://www.cnblogs.com/tornadomeet/p/3432093.html" title="发布于2013-11-19 19:11">Deep learning：四十七(Stochastic Pooling简单理解)</a><br><a href="http://www.cnblogs.com/tornadomeet/p/3439503.html" class="p_n_p_prefix">» </a> 下一篇：<a href="http://www.cnblogs.com/tornadomeet/p/3439503.html" title="发布于2013-11-23 21:54">Deep learning：四十九(RNN-RBM简单理解)</a><br></div>
</div>


	<div class="postDesc">posted on <span id="post-date">2013-11-20 23:37</span> <a href="http://www.cnblogs.com/tornadomeet/">tornadomeet</a> 阅读(<span id="post_view_count">8933</span>) 评论(<span id="post_comment_count">12</span>)  <a href="https://i.cnblogs.com/EditPosts.aspx?postid=3434651" rel="nofollow">编辑</a> <a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#" onclick="AddToWz(3434651);return false;">收藏</a></div>
</div>
<script type="text/javascript">var allowComments=true,cb_blogId=110408,cb_entryId=3434651,cb_blogApp=currentBlogApp,cb_blogUserGuid='dae176a9-cc64-e111-aa3f-842b2b196315',cb_entryCreatedDate='2013/11/20 23:37:00';loadViewCount(cb_entryId);</script>

</div><a name="!comments"></a><div id="blog-comments-placeholder"><div id="comments_pager_top"></div>
<!--done-->
<br>
<b>评论:</b>
<div class="feedbackNoItems"></div>
	

		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2820821" class="layer">#1楼</a><a name="2820821" id="comment_anchor_2820821"></a>
				 <span class="comment_date">2013-11-21 00:07</span> | <a id="a_comment_author_2820821" href="http://home.cnblogs.com/u/584379/" target="_blank">winstywang</a> <a href="http://msg.cnblogs.com/send/winstywang" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2820821" class="blog_comment_body">其实很容易理解 Jacobian代表的是输入变化一点 你对应的输出的变化量，那么一个稳定的特征表示一定是对于这种微小的扰动robust的，也就是说它应该对于相似的输入保持相对一致的输出，这个motivation和denoising autoencoder是一致的</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2820821,&#39;Digg&#39;,this)">支持(2)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2820821,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2820862" class="layer">#2楼</a><a name="2820862" id="comment_anchor_2820862"></a>[<span class="louzhu">楼主</span>]
				 <span class="comment_date">2013-11-21 08:15</span> | <a id="a_comment_author_2820862" href="http://www.cnblogs.com/tornadomeet/" target="_blank">tornadomeet</a> <a href="http://msg.cnblogs.com/send/tornadomeet" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2820862" class="blog_comment_body"><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2820821" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,2820821);">@</a>
winstywang<br>谢谢！ 你帮忙解释下雅克比矩阵特征值的意义，还有那个Contractive Curves的图到底在讲什么呢？</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2820862,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2820862,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2844313" class="layer">#3楼</a><a name="2844313" id="comment_anchor_2844313"></a>
				 <span class="comment_date">2013-12-25 15:22</span> | <a id="a_comment_author_2844313" href="http://home.cnblogs.com/u/572074/" target="_blank">xinquan007</a> <a href="http://msg.cnblogs.com/send/xinquan007" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2844313" class="blog_comment_body">雅可比矩阵为输出层对输入层的偏导。如果这个矩阵每个元素值都一样，则相当于对训练数据进行了缩放的效果，这样的特征提取是没有意义的。雅可比矩阵特征值可以联系PCA进行思考，特征值大的对应的空间映射后的方差比较大（即数据潜在的低维流行）。在进行特征提取时，整个模型应该尽量去拟合这个低维流行上的数据，而忽略与这个低维流行中之外的数据（contractive的作用）。</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2844313,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2844313,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2844616" class="layer">#4楼</a><a name="2844616" id="comment_anchor_2844616"></a>[<span class="louzhu">楼主</span>]
				 <span class="comment_date">2013-12-25 23:40</span> | <a id="a_comment_author_2844616" href="http://www.cnblogs.com/tornadomeet/" target="_blank">tornadomeet</a> <a href="http://msg.cnblogs.com/send/tornadomeet" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2844616" class="blog_comment_body"><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2844313" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,2844313);">@</a>
xinquan007<br>谢谢你的解释。有点收获</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2844616,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2844616,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2855013" class="layer">#5楼</a><a name="2855013" id="comment_anchor_2855013"></a>
				 <span class="comment_date">2014-01-08 09:35</span> | <a id="a_comment_author_2855013" href="http://home.cnblogs.com/u/546855/" target="_blank">bigiceberg_</a> <a href="http://msg.cnblogs.com/send/bigiceberg_" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2855013" class="blog_comment_body"><fieldset class="comment_quote"><legend>引用</legend>gparams = T.grad(cost, self.params) #Theano特有的功能，自动求导</fieldset><br><br>想问一下，CAE惩罚项的导数有解析解么？</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2855013,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2855013,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2855256" class="layer">#6楼</a><a name="2855256" id="comment_anchor_2855256"></a>[<span class="louzhu">楼主</span>]
				 <span class="comment_date">2014-01-08 11:29</span> | <a id="a_comment_author_2855256" href="http://www.cnblogs.com/tornadomeet/" target="_blank">tornadomeet</a> <a href="http://msg.cnblogs.com/send/tornadomeet" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2855256" class="blog_comment_body"><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2855013" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,2855013);">@</a>
bigiceberg_<br>有</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2855256,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2855256,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2855270" class="layer">#7楼</a><a name="2855270" id="comment_anchor_2855270"></a>
				 <span class="comment_date">2014-01-08 11:36</span> | <a id="a_comment_author_2855270" href="http://home.cnblogs.com/u/546855/" target="_blank">bigiceberg_</a> <a href="http://msg.cnblogs.com/send/bigiceberg_" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2855270" class="blog_comment_body"><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2855256" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,2855256);">@</a>
tornadomeet<br>求CAE惩罚项的对Wij的求导公式</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2855270,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2855270,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2855291" class="layer">#8楼</a><a name="2855291" id="comment_anchor_2855291"></a>[<span class="louzhu">楼主</span>]
				 <span class="comment_date">2014-01-08 11:49</span> | <a id="a_comment_author_2855291" href="http://www.cnblogs.com/tornadomeet/" target="_blank">tornadomeet</a> <a href="http://msg.cnblogs.com/send/tornadomeet" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2855291" class="blog_comment_body"><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2855270" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,2855270);">@</a>
bigiceberg_<br>自己动手去求吧。<br>博文中第4个公式中把hi用wij表示，求wij处的导数时只剩下了wij和输入x变量了，应该不难。</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2855291,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2855291,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2896496" class="layer">#9楼</a><a name="2896496" id="comment_anchor_2896496"></a>
				 <span class="comment_date">2014-03-16 19:32</span> | <a id="a_comment_author_2896496" href="http://home.cnblogs.com/u/533835/" target="_blank">tjming</a> <a href="http://msg.cnblogs.com/send/tjming" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2896496" class="blog_comment_body">CAE貌似是Convolutional Auto-Encoders</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2896496,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2896496,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#3568261" class="layer">#10楼</a><a name="3568261" id="comment_anchor_3568261"></a>
				 <span class="comment_date">2016-11-29 21:10</span> | <a id="a_comment_author_3568261" href="http://home.cnblogs.com/u/1072253/" target="_blank">qiyumeimei</a> <a href="http://msg.cnblogs.com/send/qiyumeimei" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_3568261" class="blog_comment_body">你好，我想问一下那个雅克比矩阵是不是只针对编码网络部分？</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(3568261,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(3568261,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#3568499" class="layer">#11楼</a><a name="3568499" id="comment_anchor_3568499"></a>
				 <span class="comment_date">2016-11-30 09:55</span> | <a id="a_comment_author_3568499" href="http://home.cnblogs.com/u/1072253/" target="_blank">qiyumeimei</a> <a href="http://msg.cnblogs.com/send/qiyumeimei" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_3568499" class="blog_comment_body"><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#2855270" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,2855270);">@</a>
bigiceberg_<br>请问你解决那个求导的问题了吗？是不是用BP算法求残差再求导？</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(3568499,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(3568499,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#3574399" class="layer">#12楼</a><a name="3574399" id="comment_anchor_3574399"></a><span id="comment-maxId" style="display:none;">3574399</span><span id="comment-maxDate" style="display:none;">2016/12/7 10:10:57</span>
				 <span class="comment_date">2016-12-07 10:10</span> | <a id="a_comment_author_3574399" href="http://home.cnblogs.com/u/1072253/" target="_blank">qiyumeimei</a> <a href="http://msg.cnblogs.com/send/qiyumeimei" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_3574399" class="blog_comment_body">博主，紧急求教啊，我主要研究故障诊断方面，但是利用CAE提取特征时发现各种类型的故障提取的特征是一样的，所以我的分类精度只有25%，求指教</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(3574399,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(3574399,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	



<div id="comments_pager_bottom"></div></div><script type="text/javascript">var commentManager = new blogCommentManager();commentManager.renderComments(0);</script>
<div id="comment_form" class="commentform">
<a name="commentform"></a>
<div id="divCommentShow"></div>
<div id="comment_nav"><span id="span_refresh_tips"></span><a href="javascript:void(0);" onclick="return RefreshCommentList();" id="lnk_RefreshComments" runat="server" clientidmode="Static">刷新评论</a><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#" onclick="return RefreshPage();">刷新页面</a><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#top">返回顶部</a></div>
<div id="comment_form_container"><div class="login_tips">注册用户登录后才能发表评论，请 <a rel="nofollow" href="javascript:void(0);" class="underline" onclick="return login(&#39;commentform&#39;);">登录</a> 或 <a rel="nofollow" href="javascript:void(0);" class="underline" onclick="return register();">注册</a>，<a href="http://www.cnblogs.com/">访问</a>网站首页。</div></div>
<div class="ad_text_commentbox" id="ad_text_under_commentbox"></div>
<div id="ad_t2"><a href="http://www.ucancode.com/index.htm" target="_blank">【推荐】50万行VC++源码: 大型组态工控、电力仿真CAD与GIS源码库</a><br><a href="http://arcapp.anruichina.com/arctrac/trac?tid=smb_azure_1212_CNBlog" target="_blank">【福利】微软Azure给博客园的你专属双重好礼</a><br><a href="http://rongcloud.cn/reports/journal2" target="_blank">【推荐】融云发布 App 社交化白皮书 IM 提升活跃超 8 倍</a><br><a href="http://bbs.h3bpm.com/index.php?m=app&amp;app=product_download&amp;a=reg" target="_blank">【推荐】BPM免费下载</a><br></div>
<div id="opt_under_post"></div>
<div id="ad_c1" class="c_ad_block"><a href="http://www.gcpowertools.com.cn/products/componentone.htm?utm_source=cnblogs&amp;utm_medium=blogpage&amp;utm_term=bottom&amp;utm_content=C1&amp;utm_campaign=community" target="_blank"><img width="300" height="250" src="./Contractive AutoEncoder_files/24442-20161031104644908-57254170.png" alt=""></a></div>
<div id="under_post_news"><div class="itnews c_ad_block"><b>最新IT新闻</b>:<br> ·  <a href="http://news.cnblogs.com/n/559581/" target="_blank">缺乏核心专利，未来中国石墨烯企业或将造集体诉讼</a><br> ·  <a href="http://news.cnblogs.com/n/559580/" target="_blank">抵抗雾霾行动：2016能源创新大事记，苹果、Google领衔</a><br> ·  <a href="http://news.cnblogs.com/n/559579/" target="_blank">Snapchat的这些新滤镜，你只有坐在Uber上才能用</a><br> ·  <a href="http://news.cnblogs.com/n/559578/" target="_blank">那个特牛的黑客团，这次黑了蚁人、美国队长、奇异博士的推</a><br> ·  <a href="http://news.cnblogs.com/n/559576/" target="_blank">互联网造车军团再添猛将：中兴宣布进军电动汽车</a><br>» <a href="http://news.cnblogs.com/" title="IT新闻" target="_blank">更多新闻...</a></div></div>
<div id="ad_c2" class="c_ad_block"><a href="http://bbs.h3bpm.com/index.php?m=app&amp;app=product_download&amp;a=reg" target="_blank"><img width="468" height="60" src="./Contractive AutoEncoder_files/35695-20161213142353073-1602158633.jpg" alt=""></a></div>
<div id="under_post_kb"><div class="itnews c_ad_block" id="kb_block"><b>最新知识库文章</b>:<br><div id="kb_recent"> ·  <a href="http://kb.cnblogs.com/page/556770/" target="_blank">写给未来的程序媛</a><br> ·  <a href="http://kb.cnblogs.com/page/558087/" target="_blank">高质量的工程代码为什么难写</a><br> ·  <a href="http://kb.cnblogs.com/page/555750/" target="_blank">循序渐进地代码重构</a><br> ·  <a href="http://kb.cnblogs.com/page/554496/" target="_blank">技术的正宗与野路子</a><br> ·  <a href="http://kb.cnblogs.com/page/553682/" target="_blank">陈皓：什么是工程师文化？</a><br></div>» <a href="http://kb.cnblogs.com/" target="_blank">更多知识库文章...</a></div></div>
<div id="HistoryToday" class="c_ad_block"></div>
<script type="text/javascript">
    fixPostBody();
    setTimeout(function () { incrementViewCount(cb_entryId); }, 50);
    deliverAdT2();
    deliverAdC1();
    deliverAdC2();    
    loadNewsAndKb();
    loadBlogSignature();
    LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);
    GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate);
    loadOptUnderPost();
    GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);   
</script>
</div>


</div>
<div id="leftcontent">
	
		<div id="leftcontentcontainer">
			
<!--done-->
<div class="newsItem">
	<div id="blog-news"><div id="profile_block">昵称：<a href="http://home.cnblogs.com/u/tornadomeet/">tornadomeet</a><br>园龄：<a href="http://home.cnblogs.com/u/tornadomeet/" title="入园时间：2012-03-03">4年9个月</a><br>粉丝：<a href="http://home.cnblogs.com/u/tornadomeet/followers/">3284</a><br>关注：<a href="http://home.cnblogs.com/u/tornadomeet/followees/">46</a><div id="p_b_follow"><a href="javascript:void(0);" onclick="follow(&#39;dae176a9-cc64-e111-aa3f-842b2b196315&#39;)">+加关注</a></div></div></div><script type="text/javascript">loadBlogNews();</script>
</div>

			<div id="blog-calendar" style=""><table id="blogCalendar" class="Cal" cellspacing="0" cellpadding="0" title="日历">
	<tbody><tr><td colspan="7"><table class="CalTitle" cellspacing="0">
		<tbody><tr><td class="CalNextPrev"><a href="javascript:void(0);" onclick="loadBlogCalendar(&#39;2016/11/01&#39;);return false;">&lt;</a></td><td align="center">2016年12月</td><td class="CalNextPrev" align="right"><a href="javascript:void(0);" onclick="loadBlogCalendar(&#39;2017/01/01&#39;);return false;">&gt;</a></td></tr>
	</tbody></table></td></tr><tr><th class="CalDayHeader" align="center" abbr="日" scope="col">日</th><th class="CalDayHeader" align="center" abbr="一" scope="col">一</th><th class="CalDayHeader" align="center" abbr="二" scope="col">二</th><th class="CalDayHeader" align="center" abbr="三" scope="col">三</th><th class="CalDayHeader" align="center" abbr="四" scope="col">四</th><th class="CalDayHeader" align="center" abbr="五" scope="col">五</th><th class="CalDayHeader" align="center" abbr="六" scope="col">六</th></tr><tr><td class="CalOtherMonthDay" align="center">27</td><td class="CalOtherMonthDay" align="center">28</td><td class="CalOtherMonthDay" align="center">29</td><td class="CalOtherMonthDay" align="center">30</td><td align="center">1</td><td align="center">2</td><td class="CalWeekendDay" align="center">3</td></tr><tr><td class="CalWeekendDay" align="center">4</td><td align="center">5</td><td align="center">6</td><td align="center">7</td><td align="center">8</td><td align="center">9</td><td class="CalWeekendDay" align="center">10</td></tr><tr><td class="CalWeekendDay" align="center">11</td><td align="center">12</td><td align="center">13</td><td align="center">14</td><td align="center">15</td><td align="center">16</td><td class="CalWeekendDay" align="center">17</td></tr><tr><td class="CalWeekendDay" align="center">18</td><td align="center">19</td><td align="center">20</td><td align="center">21</td><td class="CalTodayDay" align="center">22</td><td align="center">23</td><td class="CalWeekendDay" align="center">24</td></tr><tr><td class="CalWeekendDay" align="center">25</td><td align="center">26</td><td align="center">27</td><td align="center">28</td><td align="center">29</td><td align="center">30</td><td class="CalWeekendDay" align="center">31</td></tr><tr><td class="CalOtherMonthDay" align="center">1</td><td class="CalOtherMonthDay" align="center">2</td><td class="CalOtherMonthDay" align="center">3</td><td class="CalOtherMonthDay" align="center">4</td><td class="CalOtherMonthDay" align="center">5</td><td class="CalOtherMonthDay" align="center">6</td><td class="CalOtherMonthDay" align="center">7</td></tr>
</tbody></table></div><script type="text/javascript">loadBlogDefaultCalendar();</script><br>
			<div id="blog-sidecolumn"><div id="sidebar_search" class="sidebar-block">
<div id="sidebar_search" class="mySearch">
<h3 class="catListTitle">搜索</h3>
<div id="sidebar_search_box">
<div id="widget_my_zzk" class="div_my_zzk"><input type="text" id="q" onkeydown="return zzk_go_enter(event);" class="input_my_zzk">&nbsp;<input onclick="zzk_go()" type="button" value="找找看" id="btnZzk" class="btn_my_zzk"></div>
<div id="widget_my_google" class="div_my_zzk"><input type="text" name="google_q" id="google_q" onkeydown="return google_go_enter(event)" class="input_my_zzk">&nbsp;<input onclick="google_go()" type="button" value="谷歌搜索" class="btn_my_zzk"></div>
</div>
</div>

</div><div id="sidebar_shortcut" class="sidebar-block">
<h3 class="catListTitle">常用链接</h3>
<ul>
<li><a href="http://www.cnblogs.com/tornadomeet/p/" title="我的博客的随笔列表">我的随笔</a></li><li><a href="http://www.cnblogs.com/tornadomeet/MyComments.html" title="我发表过的评论列表">我的评论</a></li><li><a href="http://www.cnblogs.com/tornadomeet/OtherPosts.html" title="我评论过的随笔列表">我的参与</a></li><li><a href="http://www.cnblogs.com/tornadomeet/RecentComments.html" title="我的博客的评论列表">最新评论</a></li><li><a href="http://www.cnblogs.com/tornadomeet/tag/" title="我的博客的标签列表">我的标签</a></li>
</ul>
<div id="itemListLin_con" style="display:none;">

</div></div><div id="sidebar_toptags" class="sidebar-block">
<h3 class="catListTitle">我的标签</h3>
<div id="MyTag">
<ul>
<li><a href="http://www.cnblogs.com/tornadomeet/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>(72)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/Deep%20Learning/">Deep Learning</a>(51)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/opencv/">opencv</a>(34)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8Bopencv/">基础学习笔记之opencv</a>(24)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/Android%E5%BC%80%E5%8F%91%E5%8E%86%E7%A8%8B/">Android开发历程</a>(18)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/matlab/">matlab</a>(16)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/reading%20papers/">reading papers</a>(16)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/%E6%80%BB%E7%BB%93%E7%B3%BB%E5%88%97/">总结系列</a>(15)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/Qt%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/">Qt学习之路</a>(14)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/OpenNI/">OpenNI</a>(14)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/">更多</a></li>
</ul>
</div></div><div id="sidebar_categories">
		<h3 class="catListTitle">随笔分类<span style="font-size:11px;font-weight:normal">(468)</span></h3>
		
				<ul class="catList">
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_0" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/400063.html">Android(19)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_1" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/362086.html">ARM</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_2" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/362087.html">C/C++(6)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_3" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361470.html">CV(47)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_4" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/497607.html">Deep Learning(51)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_5" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361469.html">DIP(7)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_6" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/437087.html">Eigen(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_7" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361817.html">FPGA</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_8" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/366103.html">IR(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_9" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/400062.html">Java</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_10" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/416316.html">Kinect(15)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_11" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361818.html">Linux(2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_12" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361467.html">matlab(17)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_13" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361466.html">OpenCV(57)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_14" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/406166.html">OpenGL(7)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_15" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/416317.html">OpenNI(14)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_16" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/379510.html">Paper(8)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_17" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/374732.html">Qt(36)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_18" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361814.html">Robot(2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_19" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/402881.html">XML(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_20" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361815.html">单片机</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_21" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361816.html">电子设计</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_22" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/599067.html">感悟总结(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_23" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361811.html">机器学习(91)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_24" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/371256.html">计算机网络(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_25" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/375969.html">控制理论(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_26" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361812.html">模式识别(11)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_27" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361819.html">嵌入式</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_28" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361813.html">人工智能(8)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_29" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/362088.html">神经网络(2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_30" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/444797.html">手势识别(3)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_31" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/426653.html">数据结构(6)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_32" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/489160.html">数据挖掘(13)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_33" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/370489.html">数学(2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_34" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/362085.html">数字信号处理(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_35" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361810.html">算法(7)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_36" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/366102.html">语音处理(4)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_37" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/376567.html">总结(24)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_38" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/370949.html">最优化(2)</a></li>
			
				</ul>
			
	
		<h3 class="catListTitle">随笔档案<span style="font-size:11px;font-weight:normal">(252)</span></h3>
		
				<ul class="catList">
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_0" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2014/07.html">2014年7月 (1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_1" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2014/01.html">2014年1月 (6)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_2" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/12.html">2013年12月 (3)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_3" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/11.html">2013年11月 (7)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_4" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/10.html">2013年10月 (1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_5" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/09.html">2013年9月 (2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_6" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/08.html">2013年8月 (6)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_7" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/07.html">2013年7月 (2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_8" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/06.html">2013年6月 (5)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_9" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/05.html">2013年5月 (8)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_10" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/04.html">2013年4月 (18)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_11" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/03.html">2013年3月 (20)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_12" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/02.html">2013年2月 (2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_13" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/01.html">2013年1月 (4)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_14" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/12.html">2012年12月 (15)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_15" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/11.html">2012年11月 (15)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_16" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/10.html">2012年10月 (8)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_17" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/09.html">2012年9月 (11)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_18" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/08.html">2012年8月 (24)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_19" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/07.html">2012年7月 (29)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_20" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/06.html">2012年6月 (15)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_21" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/05.html">2012年5月 (14)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_22" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/04.html">2012年4月 (14)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_23" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/03.html">2012年3月 (22)</a></li>
			
				</ul>
			
	
		<h3 class="catListTitle">文章分类</h3>
		
				<ul class="catList">
			
				<li class="catListItem"> <a id="CatList_LinkList_2_Link_0" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/599066.html">感悟总结</a></li>
			
				</ul>
			
	
</div><div id="sidebar_scorerank" class="sidebar-block">
<h3>积分与排名</h3>
<ul>
	<li>
		积分 -
		700769
	</li><li>
		排名 -
		120
	</li>
</ul>
</div><div id="sidebar_recentcomments" class="sidebar-block"><div id="recent_comments_wrap">
<h3 class="catListTitle">最新评论</h3>
<div class="RecentComment" id="RecentComments">
	<div id="RecentCommentsBlock"><ul>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html#3578206">1. Re:Deep learning：三十八(Stacked CNN简单介绍)</a></li>
        <li class="recent_comment_body">您好，“ C3层的每个特征图并不一定是都与S2层的特征图相连接，有可能只与其中的某几个连接”，这里的“某几个”是按什么样的规律呢，您又是如何选择的呢，期待您的回复</li>
        <li class="recent_comment_author">--勾勒爱之年华</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2012/05/24/2515980.html#3577217">2. Re:本人常用资源整理(ing...)</a></li>
        <li class="recent_comment_body">好人！ 厉害！</li>
        <li class="recent_comment_author">--ZaneWang</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#3574399">3. Re:Deep learning：四十八(Contractive AutoEncoder简单理解)</a></li>
        <li class="recent_comment_body">博主，紧急求教啊，我主要研究故障诊断方面，但是利用CAE提取特征时发现各种类型的故障提取的特征是一样的，所以我的分类精度只有25%，求指教</li>
        <li class="recent_comment_author">--qiyumeimei</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/16/3024292.html#3573005">4. Re:Deep learning：二十九(Sparse coding练习)</a></li>
        <li class="recent_comment_body">@wanwan0508你好，请问这个问题你拓扑结构下这个check的错误率过大的问题解决了吗？我按照楼主注释掉规则化中偏移patches均值那一步骤后，如下% Rescale from [-1,1] ......</li>
        <li class="recent_comment_author">--三山半</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html#3572155">5. Re:Deep learning：三十八(Stacked CNN简单介绍)</a></li>
        <li class="recent_comment_body">您好，我现在正在仿照DLtoolbox写CNN，工具箱中我采用了您说的“其中打X了的表示两者之间有连接的”，但是在反向BP的时候，该怎么样进行反向BP的传播呢。</li>
        <li class="recent_comment_author">--勾勒爱之年华</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/19/2694332.html#3571293">6. Re:Qt学习之路_12(简易数据管理系统)</a></li>
        <li class="recent_comment_body">求源码 1224373565@qq.com</li>
        <li class="recent_comment_author">--漫步云海</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#3568499">7. Re:Deep learning：四十八(Contractive AutoEncoder简单理解)</a></li>
        <li class="recent_comment_body">@bigiceberg_请问你解决那个求导的问题了吗？是不是用BP算法求残差再求导？...</li>
        <li class="recent_comment_author">--qiyumeimei</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#3568261">8. Re:Deep learning：四十八(Contractive AutoEncoder简单理解)</a></li>
        <li class="recent_comment_body">你好，我想问一下那个雅克比矩阵是不是只针对编码网络部分？</li>
        <li class="recent_comment_author">--qiyumeimei</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/22/2975978.html#3560796">9. Re:Deep learning：十三(Softmax Regression)</a></li>
        <li class="recent_comment_body">损失函数中指示函数"1{.}"写错了吧。。。应该为“0{.}”？<br>损失函数不是应该在预测正确时不惩罚，预测错误时惩罚么。。貌似写反了。</li>
        <li class="recent_comment_author">--欧麦高德</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/28/2420936.html#3560745">10. Re:基础学习笔记之opencv(3)：haartraining生成.xml文件过程</a></li>
        <li class="recent_comment_body">博主您好，我在生成.vec文件时，出现了pos.txt(1): parse errorDone. Create 0 samples 请问这是什么问题呢？命令如下：-info pos.txt -vec......</li>
        <li class="recent_comment_author">--zyriris</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/20/2970724.html#3559683">11. Re:Deep learning：九(Sparse Autoencoder练习)</a></li>
        <li class="recent_comment_body">想请教几个问题，谁能加我的qq：9315387，谢谢！另外，最后显示的是权重w，显示w有什么意义？不如显示特征</li>
        <li class="recent_comment_author">--admudzl</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/p/3468450.html#3557512">12. Re:Deep learning：五十一(CNN的反向求导及练习)</a></li>
        <li class="recent_comment_body">楼主你好 ufldl上说在更新参数的时候要在20分钟之内 我的程序跑了40分钟 请问有什么最值得优化的地方 或者说最耗时的地方</li>
        <li class="recent_comment_author">--kimir17</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/19/2970101.html#3557043">13. Re:Deep learning：八(Sparse Autoencoder)</a></li>
        <li class="recent_comment_body">“其中的参数一般取很小，比如说0.05，也就是小概率发生事件的概率。这说明要求隐含层的每一个节点的输出均值接近0.05”----楼主能帮忙再解释下么，多谢~</li>
        <li class="recent_comment_author">--欧麦高德</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/18/2966041.html#3556597">14. Re:Deep learning：七(基础知识_2)</a></li>
        <li class="recent_comment_body">楼主您好。<br>请问“隐含层神经元的个数越多则效果会越好”，这个不绝对吧？</li>
        <li class="recent_comment_author">--欧麦高德</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/26/2704046.html#3554242">15. Re:Kinect+OpenNI学习笔记之1(开发环境的建立)</a></li>
        <li class="recent_comment_body">楼主，你好。我驱动安装成功后的设备管理器处会显示没有kinect motor，请问你上面的说“手动更新驱动程序到指定的安装目录”如何实现。</li>
        <li class="recent_comment_author">--骋</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/28/2568634.html#3554202">16. Re:Qt学习之路_4(Qt UDP的初步使用)</a></li>
        <li class="recent_comment_body">@kyww我也出现过这种情况，不过解决了，你现在解决了吗...</li>
        <li class="recent_comment_author">--彷徨中前行的我</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/14/2959138.html#3553429">17. Re:Deep learning：一(基础知识_1)</a></li>
        <li class="recent_comment_body">您好，请问文中“牛顿法不需要选择任何参数”怎么理解？？多谢~~</li>
        <li class="recent_comment_author">--欧麦高德</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/24/2979408.html#3537905">18. Re:Deep learning：十五(Self-Taught Learning练习)</a></li>
        <li class="recent_comment_body">请问博主，对于这种self taught learning的hiddensize怎么确定？ 我自己用1000个数据试了一下：当hiddensize=200，准确率=90%；当hiddensize = ......</li>
        <li class="recent_comment_author">--zoey321</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/p/3258122.html#3535043">19. Re:Deep learning：四十一(Dropout简单理解)</a></li>
        <li class="recent_comment_body">博文里提到的native bayes，应该是naive bayes（朴素贝叶斯）。看了下论文原文，dropout可以看作是bagging的一个特例，博主这里提到的boosting应该是笔误吧，boos......</li>
        <li class="recent_comment_author">--ChrisZZ</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/p/3468450.html#3534132">20. Re:Deep learning：五十一(CNN的反向求导及练习)</a></li>
        <li class="recent_comment_body">@tornadomeet您好，我想请问一下，怎么换成自己的数据，我想看看效果，希望能指点，谢谢！...</li>
        <li class="recent_comment_author">--susanwq</li>
</ul>
</div>
</div>
</div></div><div id="sidebar_topviewedposts" class="sidebar-block"><div id="topview_posts_wrap">
<h3 class="catListTitle">阅读排行榜</h3>
<div class="RecentComment" id="TopViewPosts"> 
	<div id="TopViewPostsBlock"><ul><li><a href="http://www.cnblogs.com/tornadomeet/p/3395593.html">1. 机器学习&amp;数据挖掘笔记_16（常见面试之机器学习算法思想简单梳理）(91265)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3468450.html">2. Deep learning：五十一(CNN的反向求导及练习)(66030)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/14/2959138.html">3. Deep learning：一(基础知识_1)(65199)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/27/2984725.html">4. Deep learning：十九(RBM简单理解)(58826)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3439503.html">5. Deep learning：四十九(RNN-RBM简单理解)(57577)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html">6. Deep learning：三十八(Stacked CNN简单介绍)(57283)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3258122.html">7. Deep learning：四十一(Dropout简单理解)(56368)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/22/2975978.html">8. Deep learning：十三(Softmax Regression)(53408)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/12/26/2834336.html">9. 基础学习笔记之opencv(24)：imwrite函数的使用(53190)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/17/2644903.html">10. 特征点检测学习_2(surf算法)(52174)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/15/2640754.html">11. opencv源码解析之(6)：hog源码分析(47920)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/20/2970724.html">12. Deep learning：九(Sparse Autoencoder练习)(41110)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/03/2621814.html">13. 目标检测学习_1(用opencv自带hog实现行人检测)(40423)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/15/2398769.html">14. 目标跟踪学习笔记_1(opencv中meanshift和camshift例子的应用)(39390)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/05/24/2515980.html">15. 本人常用资源整理(ing...)(36167)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/22/2651574.html">16. OpenGL_Qt学习笔记之_01(创建一个OpenGL窗口)(34134)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/06/2538695.html">17. 图像分割学习笔记_1(opencv自带meanshift分割例子)(33818)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/06/2673104.html">18. PCA算法学习_1(OpenCV中PCA实现人脸降维)(33691)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/23/2699077.html">19. Qt学习之路_14(简易音乐播放器)(32751)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3261247.html">20. Deep learning：四十二(Denoise Autoencoder简单理解)(32384)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/22/2411318.html">21. 基础学习笔记之opencv(1)：opencv中facedetect例子浅析(32368)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/19/2970101.html">22. Deep learning：八(Sparse Autoencoder)(31880)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/12/30/2839615.html">23. PCA算法学习_2(PCA理论的matlab实现)(30921)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/28/2568634.html">24. Qt学习之路_4(Qt UDP的初步使用)(29482)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/13/3018393.html">25. Deep learning：二十六(Sparse coding简单理解)(29299)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/16/2643168.html">26. 特征点检测学习_1(sift算法)(29277)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/09/3009830.html">27. Deep learning：二十三(Convolution和Pooling练习)(28122)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/15/2961660.html">28. Deep learning：二(linear regression练习)(28058)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/24/2560261.html">29. 本人部分博客导航(ing...)(27755)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/23/2413363.html">30. HMM学习笔记_1(从一个实例中学习DTW算法)(27281)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/11/06/2756361.html">31. 一些知识点的初步理解_7(随机森林,ing...)(26122)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/19/2694332.html">32. Qt学习之路_12(简易数据管理系统)(26014)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/30/2571001.html">33. Qt学习之路_5(Qt TCP的初步使用)(25357)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/26/2982694.html">34. Deep learning：十八(关于随机采样)(23759)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/28/2420936.html">35. 基础学习笔记之opencv(3)：haartraining生成.xml文件过程(23752)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/24/2415583.html">36. HMM学习笔记_2(从一个实例中学习HMM前向算法)(23623)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/08/2384843.html">37. opencv源码解析之(3)：特征点检查前言1(23279)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/16/2963919.html">38. Deep learning：四(logistic regression练习)(23217)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/11/23/2783709.html">39. 基础学习笔记之opencv(18)：kmeans函数使用实例(22458)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/19/2646412.html">40. 目标跟踪学习笔记_5(opencv中kalman点跟踪例子)(22344)</a></li></ul></div>
</div>
</div></div><div id="sidebar_topcommentedposts" class="sidebar-block"><div id="topfeedback_posts_wrap">
<h3 class="catListTitle">评论排行榜</h3>
<div class="RecentComment" id="TopCommentsPosts">
	<div id="TopFeedbackPostsBlock"><ul><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/20/2970724.html">1. Deep learning：九(Sparse Autoencoder练习)(98)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/09/3011209.html">2. Deep learning：二十四(stacked autoencoder练习)(77)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/30/3052349.html">3. Deep learning：三十五(用NN实现数据降维练习)(73)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/23/2977621.html">4. Deep learning：十四(Softmax Regression练习)(70)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/16/3024292.html">5. Deep learning：二十九(Sparse coding练习)(66)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/11/04/2753185.html">6. Kinect+OpenNI学习笔记之12(简单手势所表示的数字的识别)(57)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/24/2979408.html">7. Deep learning：十五(Self-Taught Learning练习)(55)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/28/2420936.html">8. 基础学习笔记之opencv(3)：haartraining生成.xml文件过程(53)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/09/3009830.html">9. Deep learning：二十三(Convolution和Pooling练习)(53)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html">10. Deep learning：三十八(Stacked CNN简单介绍)(53)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3468450.html">11. Deep learning：五十一(CNN的反向求导及练习)(44)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/24/2415889.html">12. HMM学习笔记_3(从一个实例中学习Viterbi算法)(38)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/08/2384843.html">13. opencv源码解析之(3)：特征点检查前言1(32)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/10/18/2728896.html">14. Kinect+OpenNI学习笔记之8(Robert Walter手部提取代码的分析)(32)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/17/2644903.html">15. 特征点检测学习_2(surf算法)(31)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/22/2411318.html">16. 基础学习笔记之opencv(1)：opencv中facedetect例子浅析(31)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/10/19/2730891.html">17. Kinect+OpenNI学习笔记之9(不需要骨骼跟踪的人体手部分割)(30)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/15/2640754.html">18. opencv源码解析之(6)：hog源码分析(29)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/08/3007435.html">19. Deep learning：二十二(linear decoder练习)(29)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3874378.html">20. 告别学生时代(28)</a></li></ul></div>
</div></div></div><div id="sidebar_topdiggedposts" class="sidebar-block"><div id="topdigg_posts_wrap">
<h3 class="catListTitle">推荐排行榜</h3>
<div class="RecentComment">
	<div id="TopDiggPostsBlock"><ul><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/05/24/2515980.html">1. 本人常用资源整理(ing...)(36)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3395593.html">2. 机器学习&amp;数据挖掘笔记_16（常见面试之机器学习算法思想简单梳理）(25)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/14/2959138.html">3. Deep learning：一(基础知识_1)(18)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/15/2640754.html">4. opencv源码解析之(6)：hog源码分析(16)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html">5. Deep learning：三十八(Stacked CNN简单介绍)(11)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/24/2560261.html">6. 本人部分博客导航(ing...)(11)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/30/3052349.html">7. Deep learning：三十五(用NN实现数据降维练习)(8)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/11/04/2753185.html">8. Kinect+OpenNI学习笔记之12(简单手势所表示的数字的识别)(8)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3258122.html">9. Deep learning：四十一(Dropout简单理解)(7)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/11/12/2766458.html">10. 龙星计划机器学习笔记(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/23/2699077.html">11. Qt学习之路_14(简易音乐播放器)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/22/2698337.html">12. Qt学习之路_13(简易俄罗斯方块)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/19/2694332.html">13. Qt学习之路_12(简易数据管理系统)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/17/2644903.html">14. 特征点检测学习_2(surf算法)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3874378.html">15. 告别学生时代(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3468450.html">16. Deep learning：五十一(CNN的反向求导及练习)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/02/2531565.html">17. 前景检测算法_3(GMM)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/15/2398769.html">18. 目标跟踪学习笔记_1(opencv中meanshift和camshift例子的应用)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/24/2415889.html">19. HMM学习笔记_3(从一个实例中学习Viterbi算法)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/04/12/2443993.html">20. 初步体验libsvm用法1(官方自带工具)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/24/2415583.html">21. HMM学习笔记_2(从一个实例中学习HMM前向算法)(5)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/27/2420088.html">22. 基础学习笔记之opencv(2)：haartraining前将统一图片尺寸方法(5)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/20/2408086.html">23. Matlab DIP(瓦)ch9形态学图像处理(5)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/16/2643168.html">24. 特征点检测学习_1(sift算法)(5)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3276753.html">25. 机器学习&amp;数据挖掘笔记_14（GMM-HMM语音识别简单理解）(5)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3439503.html">26. Deep learning：四十九(RNN-RBM简单理解)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/27/2706417.html">27. Kinect+OpenNI学习笔记之2(获取kinect的颜色图像和深度图像)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/11/09/2763271.html">28. 基础学习笔记之opencv(16)：grabcut使用例程(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/12/30/2839615.html">29. PCA算法学习_2(PCA理论的matlab实现)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/15/2961660.html">30. Deep learning：二(linear regression练习)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/12/12/2813939.html">31. 基础学习笔记之opencv(23)：OpenCV坐标体系的初步认识(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/12/04/2800701.html">32. 基础学习笔记之opencv(20)：OpenCV中的颜色空间(ing...)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/23/2413363.html">33. HMM学习笔记_1(从一个实例中学习DTW算法)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/03/2621814.html">34. 目标检测学习_1(用opencv自带hog实现行人检测)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/07/31/2616180.html">35. Qt学习之路_8(Qt中与文件目录相关操作)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/06/15/3137239.html">36. 机器学习&amp;数据挖掘笔记_11（高斯过程回归）(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/06/14/3135380.html">37. 机器学习&amp;数据挖掘笔记_10（高斯过程简单理解）(3)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/05/07/3065953.html">38. Deep learning：三十九(ICA模型练习)(3)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/09/3009830.html">39. Deep learning：二十三(Convolution和Pooling练习)(3)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/07/30/2615913.html">40. Qt学习之路_7(线性布局和网格布局初步探索)(3)</a></li></ul></div>
</div></div></div></div><script type="text/javascript">loadBlogSideColumn();</script></div>
	
</div>

<!--done-->
<div class="footer">
	Powered by: <a href="http://www.cnblogs.com/">博客园</a>	模板提供：<a href="http://blog.hjenglish.com/">沪江博客</a>
	Copyright ©2016 tornadomeet
</div>



<!--PageEndHtml Block Begin-->
阿萨德发斯蒂芬
<!--PageEndHtml Block End-->


</body></html>