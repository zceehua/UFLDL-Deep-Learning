<!DOCTYPE html>
<!-- saved from url=(0049)http://www.cnblogs.com/tornadomeet/p/3444128.html -->
<html lang="zh-cn"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Deep learning：五十(Deconvolution Network简单理解) - tornadomeet - 博客园</title>
<link type="text/css" rel="stylesheet" href="./Deconvolution Network简单理解_files/blog-common.css">
<link id="MainCss" type="text/css" rel="stylesheet" href="./Deconvolution Network简单理解_files/bundle-sea.css">
<link id="mobile-style" media="only screen and (max-width: 768px)" type="text/css" rel="stylesheet" href="./Deconvolution Network简单理解_files/bundle-sea-mobile.css">
<link title="RSS" type="application/rss+xml" rel="alternate" href="http://www.cnblogs.com/tornadomeet/rss">
<link title="RSD" type="application/rsd+xml" rel="EditURI" href="http://www.cnblogs.com/tornadomeet/rsd.xml">
<link type="application/wlwmanifest+xml" rel="wlwmanifest" href="http://www.cnblogs.com/tornadomeet/wlwmanifest.xml">
<script src="./Deconvolution Network简单理解_files/jquery.js.下载" type="text/javascript"></script>  
<script type="text/javascript">var currentBlogApp = 'tornadomeet', cb_enable_mathjax=false;var isLogined=false;</script>
<script src="./Deconvolution Network简单理解_files/blog-common.js.下载" type="text/javascript"></script>
</head>
<body>
<a name="top"></a>

<!--done-->
<div id="header">
	
<!--done-->
<div class="header">
	<div class="headerText">
		<a id="Header1_HeaderTitle" class="headermaintitle" href="http://www.cnblogs.com/tornadomeet/">tornadomeet</a><br>
		
	</div>
</div>

</div>

<div id="mytopmenu">
	
		<div id="mylinks"><a id="blog_nav_sitehome" class="menu" href="http://www.cnblogs.com/">博客园</a> &nbsp;
<a id="blog_nav_myhome" class="menu" href="http://www.cnblogs.com/tornadomeet/">首页</a> &nbsp;
<a id="blog_nav_newpost" class="menu" rel="nofollow" href="https://i.cnblogs.com/EditPosts.aspx?opt=1">新随笔</a> &nbsp;
<a id="blog_nav_contact" class="menu" rel="nofollow" href="https://msg.cnblogs.com/send/tornadomeet">联系</a> &nbsp;
<a id="blog_nav_rss" class="menu" href="http://www.cnblogs.com/tornadomeet/rss">订阅</a><a id="blog_nav_rss_image" href="http://www.cnblogs.com/tornadomeet/rss"><img src="./Deconvolution Network简单理解_files/xml.gif" alt="订阅"></a>&nbsp;
<a id="blog_nav_admin" class="menu" rel="nofollow" href="https://i.cnblogs.com/">管理</a>
</div>
		<div id="mystats"><div id="blog_stats">
随笔-252&nbsp;
评论-2165&nbsp;
文章-0&nbsp;
<!--trackbacks-0-->
</div></div>
	
</div>
<div id="centercontent">
	
<div id="post_detail">
<div class="post">
	<h1 class="postTitle"><a id="cb_post_title_url" class="postTitle2" href="http://www.cnblogs.com/tornadomeet/p/3444128.html">Deep learning：五十(Deconvolution Network简单理解)</a></h1>
	<div id="cnblogs_post_body"><div class="Section1">
<p>&nbsp;</p>
<p>　　深度网络结构是由多个单层网络叠加而成的，而常见的单层网络按照编码解码情况可以分为下面3类：</p>
<ol>
<li>既有encoder部分也有decoder部分：比如常见的RBM系列（由RBM可构成的DBM, DBN等），autoencoder系列(以及由其扩展的sparse autoencoder, denoise autoencoder, contractive autoencoder, saturating autoencoder等)。</li>
<li>只包含decoder部分：比如sparse coding, 和今天要讲的deconvolution network.</li>
<li>只包含encoder部分，那就是普通的feed-forward network.</li>
</ol>
<p>　　Deconvolution network的中文名字是反卷积网络，那么什么是反卷积呢？其概念从字面就很容易理解，假设A=B*C 表示的是：B和C的卷积是A，也就是说已知B和C，求A这一过程叫做卷积。那么如果已知A和B求C或者已知A和C求B，则这个过程就叫做反卷积了，deconvolution.</p>
<p>　　Deconvolution network是和convolution network(简称CNN)对应的，在CNN中，是由input image卷积feature filter得到feature map, 而在devonvolution network中，是由feature map卷积feature filter得到input image. 所以从这点看，作者强调deconvolution network是top-down是有道理的（具体可参考Zeiler的Deconvolutional networks），看下图便可知：</p>
<p>&nbsp;　　<img src="./Deconvolution Network简单理解_files/26204535-f514e7e7b6634bc5ae08201507d5aeef.png" alt=""></p>
<p>　　上图表示的是DN(deconvolution network的简称)的第一层，其输入图像是3通道的RGB图，学到的第一层特征有12个，说明每个输入通道图像都学习到了4个特征。而其中的特征图Z是由对应通道图像和特征分别卷积后再求和得到的。</p>
<p>　　本人感觉层次反卷积网络和层次卷积稀疏编码网络（Hierarchical Convolution Sparse Coding）非常相似，只是在Sparse Coding中对图像的分解采用的是矩阵相乘的方式，而在DN这里采用的是矩阵卷积的形式。和Sparse coding中train过程交叉优化基图像和组合系数的类似，DN中每次train时也需要交叉优化feature filter和feature map.</p>
<p>　　<strong><span style="color: #0000ff;">DN的train过程：</span></strong></p>
<p>　　学习DN中第l(小写的L)层网络的特征时，需优化下面的目标函数：</p>
<p>&nbsp;　　<img src="./Deconvolution Network简单理解_files/26204628-2ce8a41c329143019adad0437b13f803.png" alt="" width="422" height="129"></p>
<p>　　它是将第l层网络的输出当做第l+1层网络的输入（这和通常的deep network训练过程类似），其中的&nbsp;<img style="line-height: 1.5;" src="./Deconvolution Network简单理解_files/26204647-5046665f90144da9bdcc3c6257c04273.png" alt="" width="48" height="31"><span style="line-height: 1.5;">&nbsp;表示第l层的特征图k和第l-1层的特征图c的连接情况，如果连接则为1,否则为0. 对上面loss函数优化的思想大致为：</span></p>
<ol>
<li>固定&nbsp;&nbsp;<img src="./Deconvolution Network简单理解_files/26204655-09f068fa50aa40d5b87c60f2ef2ca42b.png" alt="" width="38" height="31">&nbsp;，优化&nbsp;<img src="./Deconvolution Network简单理解_files/26204751-33e984c2fb6c4eba83f6f0484ecd99c8.png" alt="" width="41" height="31"> &nbsp;，但是这样不能直接优化（没弄清楚原因，可参考博客下面网友的评论），因此作者引入了一个辅助变量&nbsp;<img src="./Deconvolution Network简单理解_files/26204821-2062558aa7ac49a28b363a0679ac32a7.png" alt=""> ，则这时的loss函数变为：</li>
</ol>
<p>&nbsp;　　<img src="./Deconvolution Network简单理解_files/26205439-1fc18429624945b0987b3c7280d31fb8.png" alt="" width="415" height="123"></p>
<p>&nbsp;&nbsp; 　上式loss函数中对辅助变量&nbsp;<img src="./Deconvolution Network简单理解_files/26205556-6f9c35fe3bcf4dfabb6f83cce0d1aa28.png" alt="" width="42" height="30"> 和&nbsp;<img src="./Deconvolution Network简单理解_files/26204751-33e984c2fb6c4eba83f6f0484ecd99c8.png" alt="" width="41" height="31"> 之间的距离进行了惩罚，因此这个辅助变量的引入是合理的，接着交替优化&nbsp; 和&nbsp; <img src="./Deconvolution Network简单理解_files/26204751-33e984c2fb6c4eba83f6f0484ecd99c8.png" alt="" width="41" height="31">，直到&nbsp;&nbsp;<img src="./Deconvolution Network简单理解_files/26204751-33e984c2fb6c4eba83f6f0484ecd99c8.png" alt="" width="41" height="31">&nbsp; 收敛（具体可参考文章公式细节）。</p>
<p>　　2. 固定&nbsp;<img src="./Deconvolution Network简单理解_files/26204751-33e984c2fb6c4eba83f6f0484ecd99c8.png" alt="" width="41" height="31"> ，优化&nbsp;<img src="./Deconvolution Network简单理解_files/26205916-5f2a24c7d4614b91a13b0b24dbde3888.png" alt="" width="39" height="32"> ，直接采用梯度下降法即可。</p>
<p>　　<strong><span style="color: #0000ff;">DN的test过程：</span></strong></p>
<p>　　学习到每层网络的filter后，当输入一张新图片时，可同样采用重构误差和特征图稀疏约束来优化得到本层的feature map, 比如在第一层时，需优化：</p>
<p>&nbsp;　　<img src="./Deconvolution Network简单理解_files/26210052-7c77b3cbe2ea480e97421bd5ed062acf.png" alt=""></p>
<p>　　其中的f是在训练过程中得到的。</p>
<p>　　提取出图片y的DN特征后（可多层），可以用该特征进行图像的识别，也可以将该特征从上到下一层层卷积下来得到图像y’，而这个图像y’可理解为原图像y去噪后的图像。因此DN提取的特征至少有图像识别和图像去噪2个功能。</p>
<p>　　不难发现，如果读者对卷积稀疏编码网络熟悉的话，也就比较容易理解反卷积网络了。同理，和sparse coding一样，DA的train过程和test过程的速度都非常慢。</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp;读完这篇paper，不得不佩服搞数学优化的人。</p>
<p>&nbsp;</p>
<p>　　<span style="font-size: 18pt;"><strong><span style="color: #0000ff;">参考资料：</span></strong></span></p>
<p>　　Deconvolutional Networks, Matthew D. Zeiler, Dilip Krishnan, Graham W. Taylor and Rob Fergus.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</div></div><div id="MySignature" style="display: block;">作者：tornadomeet

出处：http://www.cnblogs.com/tornadomeet

欢迎转载或分享，但请务必声明文章出处。      （新浪微博：tornadomeet,欢迎交流！）</div>
<div class="clear"></div>
<div id="blog_post_info_block">
<div id="BlogPostCategory">分类: <a href="http://www.cnblogs.com/tornadomeet/category/497607.html" target="_blank">Deep Learning</a>,<a href="http://www.cnblogs.com/tornadomeet/category/361811.html" target="_blank">机器学习</a></div>
<div id="EntryTag">标签: <a href="http://www.cnblogs.com/tornadomeet/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>, <a href="http://www.cnblogs.com/tornadomeet/tag/Deep%20Learning/">Deep Learning</a></div>
<div id="blog_post_info"><div id="green_channel">
        <a href="javascript:void(0);" id="green_channel_digg" onclick="DiggIt(3444128,cb_blogId,1);green_channel_success(this,&#39;谢谢推荐！&#39;);">好文要顶</a>
            <a id="green_channel_follow" onclick="follow(&#39;dae176a9-cc64-e111-aa3f-842b2b196315&#39;);" href="javascript:void(0);">关注我</a>
    <a id="green_channel_favorite" onclick="AddToWz(cb_entryId);return false;" href="javascript:void(0);">收藏该文</a>
    <a id="green_channel_weibo" href="javascript:void(0);" title="分享至新浪微博" onclick="ShareToTsina()"><img src="./Deconvolution Network简单理解_files/icon_weibo_24.png" alt=""></a>
    <a id="green_channel_wechat" href="javascript:void(0);" title="分享至微信" onclick="shareOnWechat()"><img src="./Deconvolution Network简单理解_files/wechat.png" alt=""></a>
</div>
<div id="author_profile">
    <div id="author_profile_info" class="author_profile_info">
            <a href="http://home.cnblogs.com/u/tornadomeet/" target="_blank"><img src="./Deconvolution Network简单理解_files/sample_face.gif" class="author_avatar" alt=""></a>
        <div id="author_profile_detail" class="author_profile_info">
            <a href="http://home.cnblogs.com/u/tornadomeet/">tornadomeet</a><br>
            <a href="http://home.cnblogs.com/u/tornadomeet/followees">关注 - 46</a><br>
            <a href="http://home.cnblogs.com/u/tornadomeet/followers">粉丝 - 3285</a>
        </div>
    </div>
    <div class="clear"></div>
    <div id="author_profile_honor"></div>
    <div id="author_profile_follow">
                <a href="javascript:void(0);" onclick="follow(&#39;dae176a9-cc64-e111-aa3f-842b2b196315&#39;);return false;">+加关注</a>
    </div>
</div>
<div id="div_digg">
    <div class="diggit" onclick="votePost(3444128,&#39;Digg&#39;)">
        <span class="diggnum" id="digg_count">3</span>
    </div>
    <div class="buryit" onclick="votePost(3444128,&#39;Bury&#39;)">
        <span class="burynum" id="bury_count">0</span>
    </div>
    <div class="clear"></div>
    <div class="diggword" id="digg_tips">
    </div>
</div>
</div>
<div class="clear"></div>
<div id="post_next_prev"><a href="http://www.cnblogs.com/tornadomeet/p/3439503.html" class="p_n_p_prefix">« </a> 上一篇：<a href="http://www.cnblogs.com/tornadomeet/p/3439503.html" title="发布于2013-11-23 21:54">Deep learning：四十九(RNN-RBM简单理解)</a><br><a href="http://www.cnblogs.com/tornadomeet/p/3468450.html" class="p_n_p_prefix">» </a> 下一篇：<a href="http://www.cnblogs.com/tornadomeet/p/3468450.html" title="发布于2013-12-10 23:36">Deep learning：五十一(CNN的反向求导及练习)</a><br></div>
</div>


	<div class="postDesc">posted on <span id="post-date">2013-11-26 21:05</span> <a href="http://www.cnblogs.com/tornadomeet/">tornadomeet</a> 阅读(<span id="post_view_count">15594</span>) 评论(<span id="post_comment_count">10</span>)  <a href="https://i.cnblogs.com/EditPosts.aspx?postid=3444128" rel="nofollow">编辑</a> <a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#" onclick="AddToWz(3444128);return false;">收藏</a></div>
</div>
<script type="text/javascript">var allowComments=true,cb_blogId=110408,cb_entryId=3444128,cb_blogApp=currentBlogApp,cb_blogUserGuid='dae176a9-cc64-e111-aa3f-842b2b196315',cb_entryCreatedDate='2013/11/26 21:05:00';loadViewCount(cb_entryId);</script>

</div><a name="!comments"></a><div id="blog-comments-placeholder"><div id="comments_pager_top"></div>
<!--done-->
<br>
<b>评论:</b>
<div class="feedbackNoItems"></div>
	

		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#2824917" class="layer">#1楼</a><a name="2824917" id="comment_anchor_2824917"></a>
				 <span class="comment_date">2013-11-26 22:05</span> | <a id="a_comment_author_2824917" href="http://www.cnblogs.com/flytomylife/" target="_blank">flytomylife</a> <a href="http://msg.cnblogs.com/send/flytomylife" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2824917" class="blog_comment_body">关于固定fiter优化z的时候，直接优化会比较困难，貌似文章中说是：'This is due to the fact that elements in the feature maps are coupled to one another through the filters. One element in the map can be affected by another distant element, meaning that the minimization can take a very long time to converge to a good solution'. 是否指的就是，在原来的objective function中，zi是在l1正则项中的，而一个feature map 中，filter是作用与全图的patch，而如果单个的去为每个patch优化z，那么不同patch间会相互影响 -- 竞争自身（patch)的最compact的形式，然后就比较难收敛？ 所以增加auxiliary项之后，就把zi从正则项中提取出来了，也就可以同时对所有的patch同时update?</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2824917,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2824917,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#2824939" class="layer">#2楼</a><a name="2824939" id="comment_anchor_2824939"></a>[<span class="louzhu">楼主</span>]
				 <span class="comment_date">2013-11-26 22:46</span> | <a id="a_comment_author_2824939" href="http://www.cnblogs.com/tornadomeet/" target="_blank">tornadomeet</a> <a href="http://msg.cnblogs.com/send/tornadomeet" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2824939" class="blog_comment_body"><a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#2824917" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,2824917);">@</a>
flytomylife<br>恩。没加入辅助项时，原loss function很难收敛到好的点或者说需要非常长的时间才收敛，这个原因应该就是你所解释的那样，每个feature map之间有联系，会影响。不过加入辅助x后为什么就可以了，我不是很明白，感觉对x和z交叉优化时，x应该会遇到前面一样的问题。不过感觉这个搞优化的人应该比较容易理解。</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2824939,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2824939,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#2824948" class="layer">#3楼</a><a name="2824948" id="comment_anchor_2824948"></a>
				 <span class="comment_date">2013-11-26 23:00</span> | <a id="a_comment_author_2824948" href="http://www.cnblogs.com/flytomylife/" target="_blank">flytomylife</a> <a href="http://msg.cnblogs.com/send/flytomylife" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2824948" class="blog_comment_body"><a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#2824939" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,2824939);">@</a>
tornadomeet<br>感觉加入auxiliary之前，objective function中是filter和所有z都做卷积，但在引入auxiliary，优化x之后，对所有的x则没有这样的限制？</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2824948,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2824948,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#2824960" class="layer">#4楼</a><a name="2824960" id="comment_anchor_2824960"></a>[<span class="louzhu">楼主</span>]
				 <span class="comment_date">2013-11-26 23:17</span> | <a id="a_comment_author_2824960" href="http://www.cnblogs.com/tornadomeet/" target="_blank">tornadomeet</a> <a href="http://msg.cnblogs.com/send/tornadomeet" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2824960" class="blog_comment_body"><a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#2824948" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,2824948);">@</a>
flytomylife<br>应该还是有，两个表达式后面的尾巴形式貌似没有变。</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2824960,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2824960,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#2824968" class="layer">#5楼</a><a name="2824968" id="comment_anchor_2824968"></a>
				 <span class="comment_date">2013-11-26 23:37</span> | <a id="a_comment_author_2824968" href="http://www.cnblogs.com/flytomylife/" target="_blank">flytomylife</a> <a href="http://msg.cnblogs.com/send/flytomylife" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2824968" class="blog_comment_body"><a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#2824960" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,2824960);">@</a>
tornadomeet<br>感觉后面是通常的l1正则项，这个不应该是该问题中的'crux'所在。而前面x不需要对同一个filter做卷积</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2824968,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2824968,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#2824973" class="layer">#6楼</a><a name="2824973" id="comment_anchor_2824973"></a>[<span class="louzhu">楼主</span>]
				 <span class="comment_date">2013-11-26 23:44</span> | <a id="a_comment_author_2824973" href="http://www.cnblogs.com/tornadomeet/" target="_blank">tornadomeet</a> <a href="http://msg.cnblogs.com/send/tornadomeet" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_2824973" class="blog_comment_body"><a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#2824968" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,2824968);">@</a>
flytomylife<br>恩，明白了，应该就是你说的这个原因，前面的x已经代替了z，这样x和f没有直接瓜葛了。</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(2824973,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(2824973,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#3025342" class="layer">#7楼</a><a name="3025342" id="comment_anchor_3025342"></a>
				 <span class="comment_date">2014-09-09 22:26</span> | <a id="a_comment_author_3025342" href="http://home.cnblogs.com/u/670995/" target="_blank">竹林晚风</a> <a href="http://msg.cnblogs.com/send/%E7%AB%B9%E6%9E%97%E6%99%9A%E9%A3%8E" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_3025342" class="blog_comment_body">很喜欢博主的文章，刚刚用豆约翰博客备份专家备份了您的全部博文。</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(3025342,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(3025342,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#3137804" class="layer">#8楼</a><a name="3137804" id="comment_anchor_3137804"></a>
				 <span class="comment_date">2015-03-09 18:32</span> | <a id="a_comment_author_3137804" href="http://home.cnblogs.com/u/728997/" target="_blank">jacobian</a> <a href="http://msg.cnblogs.com/send/jacobian" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_3137804" class="blog_comment_body">Deconv network  感觉是一个无监督的方法，想请问如果用有监督的方式训练出一组filter和一组feature  map 应该怎么project into input呢？</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(3137804,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(3137804,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#3147422" class="layer">#9楼</a><a name="3147422" id="comment_anchor_3147422"></a>
				 <span class="comment_date">2015-03-24 11:26</span> | <a id="a_comment_author_3147422" href="http://home.cnblogs.com/u/580233/" target="_blank">boerzhu</a> <a href="http://msg.cnblogs.com/send/boerzhu" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_3147422" class="blog_comment_body"><a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#3137804" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,3137804);">@</a>
jacobian<br>可以参见作者的另外一篇论文：Visualizing and Understanding Convolutional Networks。</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(3147422,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(3147422,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
			<a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#3274132" class="layer">#10楼</a><a name="3274132" id="comment_anchor_3274132"></a><span id="comment-maxId" style="display:none;">3274132</span><span id="comment-maxDate" style="display:none;">2015/9/25 9:27:40</span>
				 <span class="comment_date">2015-09-25 09:27</span> | <a id="a_comment_author_3274132" href="http://home.cnblogs.com/u/815008/" target="_blank">赵月娇</a> <a href="http://msg.cnblogs.com/send/%E8%B5%B5%E6%9C%88%E5%A8%87" title="发送站内短消息" class="sendMsg2This">&nbsp;</a><br>
				<div align="left"><div id="comment_body_3274132" class="blog_comment_body">代码中rp(s:s+minibatch-1)，起重工rp是什么意思？</div><div class="comment_vote"><a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(3274132,&#39;Digg&#39;,this)">支持(0)</a><a href="javascript:void(0);" class="comment_bury" onclick="return voteComment(3274132,&#39;Bury&#39;,this)">反对(0)</a></div>&nbsp;&nbsp;<span class="comment_actions"></span></div>
			</div>
			
			
		</div>
	



<div id="comments_pager_bottom"></div></div><script type="text/javascript">var commentManager = new blogCommentManager();commentManager.renderComments(0);</script>
<div id="comment_form" class="commentform">
<a name="commentform"></a>
<div id="divCommentShow"></div>
<div id="comment_nav"><span id="span_refresh_tips"></span><a href="javascript:void(0);" onclick="return RefreshCommentList();" id="lnk_RefreshComments" runat="server" clientidmode="Static">刷新评论</a><a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#" onclick="return RefreshPage();">刷新页面</a><a href="http://www.cnblogs.com/tornadomeet/p/3444128.html#top">返回顶部</a></div>
<div id="comment_form_container"><div class="login_tips">注册用户登录后才能发表评论，请 <a rel="nofollow" href="javascript:void(0);" class="underline" onclick="return login(&#39;commentform&#39;);">登录</a> 或 <a rel="nofollow" href="javascript:void(0);" class="underline" onclick="return register();">注册</a>，<a href="http://www.cnblogs.com/">访问</a>网站首页。</div></div>
<div class="ad_text_commentbox" id="ad_text_under_commentbox"></div>
<div id="ad_t2"><a href="http://www.ucancode.com/index.htm" target="_blank">【推荐】50万行VC++源码: 大型组态工控、电力仿真CAD与GIS源码库</a><br><a href="http://arcapp.anruichina.com/arctrac/trac?tid=smb_azure_1212_CNBlog" target="_blank">【福利】微软Azure给博客园的你专属双重好礼</a><br><a href="http://rongcloud.cn/reports/journal2" target="_blank">【推荐】融云发布 App 社交化白皮书 IM 提升活跃超 8 倍</a><br><a href="http://bbs.h3bpm.com/index.php?m=app&amp;app=product_download&amp;a=reg" target="_blank">【推荐】BPM免费下载</a><br></div>
<div id="opt_under_post"></div>
<div id="ad_c1" class="c_ad_block"><a href="http://www.gcpowertools.com.cn/products/componentone.htm?utm_source=cnblogs&amp;utm_medium=blogpage&amp;utm_term=bottom&amp;utm_content=C1&amp;utm_campaign=community" target="_blank"><img width="300" height="250" src="./Deconvolution Network简单理解_files/24442-20161031104644908-57254170.png" alt=""></a></div>
<div id="under_post_news"><div class="itnews c_ad_block"><b>最新IT新闻</b>:<br> ·  <a href="http://news.cnblogs.com/n/559664/" target="_blank">IBM送给AI开发者的礼物：傻萌的入门级Watson机器人</a><br> ·  <a href="http://news.cnblogs.com/n/559662/" target="_blank">社交媒体亮点层出不穷，未来七大特征抢先看</a><br> ·  <a href="http://news.cnblogs.com/n/559661/" target="_blank">圣诞节，谷歌用VR带你360度欣赏纽约奢华橱窗</a><br> ·  <a href="http://news.cnblogs.com/n/559660/" target="_blank">微软承认强推Windows 10免费升级“太过激进”</a><br> ·  <a href="http://news.cnblogs.com/n/559659/" target="_blank">为了断臂求生，这次陈年卖了凡客V+</a><br>» <a href="http://news.cnblogs.com/" title="IT新闻" target="_blank">更多新闻...</a></div></div>
<div id="ad_c2" class="c_ad_block"><a href="http://bbs.h3bpm.com/index.php?m=app&amp;app=product_download&amp;a=reg" target="_blank"><img width="468" height="60" src="./Deconvolution Network简单理解_files/35695-20161213142353073-1602158633.jpg" alt=""></a></div>
<div id="under_post_kb"><div class="itnews c_ad_block" id="kb_block"><b>最新知识库文章</b>:<br><div id="kb_recent"> ·  <a href="http://kb.cnblogs.com/page/556770/" target="_blank">写给未来的程序媛</a><br> ·  <a href="http://kb.cnblogs.com/page/558087/" target="_blank">高质量的工程代码为什么难写</a><br> ·  <a href="http://kb.cnblogs.com/page/555750/" target="_blank">循序渐进地代码重构</a><br> ·  <a href="http://kb.cnblogs.com/page/554496/" target="_blank">技术的正宗与野路子</a><br> ·  <a href="http://kb.cnblogs.com/page/553682/" target="_blank">陈皓：什么是工程师文化？</a><br></div>» <a href="http://kb.cnblogs.com/" target="_blank">更多知识库文章...</a></div></div>
<div id="HistoryToday" class="c_ad_block"></div>
<script type="text/javascript">
    fixPostBody();
    setTimeout(function () { incrementViewCount(cb_entryId); }, 50);
    deliverAdT2();
    deliverAdC1();
    deliverAdC2();    
    loadNewsAndKb();
    loadBlogSignature();
    LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);
    GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate);
    loadOptUnderPost();
    GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);   
</script>
</div>


</div>
<div id="leftcontent">
	
		<div id="leftcontentcontainer">
			
<!--done-->
<div class="newsItem">
	<div id="blog-news"><div id="profile_block">昵称：<a href="http://home.cnblogs.com/u/tornadomeet/">tornadomeet</a><br>园龄：<a href="http://home.cnblogs.com/u/tornadomeet/" title="入园时间：2012-03-03">4年9个月</a><br>粉丝：<a href="http://home.cnblogs.com/u/tornadomeet/followers/">3285</a><br>关注：<a href="http://home.cnblogs.com/u/tornadomeet/followees/">46</a><div id="p_b_follow"><a href="javascript:void(0);" onclick="follow(&#39;dae176a9-cc64-e111-aa3f-842b2b196315&#39;)">+加关注</a></div></div></div><script type="text/javascript">loadBlogNews();</script>
</div>

			<div id="blog-calendar" style=""><table id="blogCalendar" class="Cal" cellspacing="0" cellpadding="0" title="Calendar">
	<tbody><tr><td colspan="7"><table class="CalTitle" cellspacing="0">
		<tbody><tr><td class="CalNextPrev"><a href="javascript:void(0);" onclick="loadBlogCalendar(&#39;2016/11/01&#39;);return false;">&lt;</a></td><td align="center">2016年12月</td><td class="CalNextPrev" align="right"><a href="javascript:void(0);" onclick="loadBlogCalendar(&#39;2017/01/01&#39;);return false;">&gt;</a></td></tr>
	</tbody></table></td></tr><tr><th class="CalDayHeader" align="center" abbr="日" scope="col">日</th><th class="CalDayHeader" align="center" abbr="一" scope="col">一</th><th class="CalDayHeader" align="center" abbr="二" scope="col">二</th><th class="CalDayHeader" align="center" abbr="三" scope="col">三</th><th class="CalDayHeader" align="center" abbr="四" scope="col">四</th><th class="CalDayHeader" align="center" abbr="五" scope="col">五</th><th class="CalDayHeader" align="center" abbr="六" scope="col">六</th></tr><tr><td class="CalOtherMonthDay" align="center">27</td><td class="CalOtherMonthDay" align="center">28</td><td class="CalOtherMonthDay" align="center">29</td><td class="CalOtherMonthDay" align="center">30</td><td align="center">1</td><td align="center">2</td><td class="CalWeekendDay" align="center">3</td></tr><tr><td class="CalWeekendDay" align="center">4</td><td align="center">5</td><td align="center">6</td><td align="center">7</td><td align="center">8</td><td align="center">9</td><td class="CalWeekendDay" align="center">10</td></tr><tr><td class="CalWeekendDay" align="center">11</td><td align="center">12</td><td align="center">13</td><td align="center">14</td><td align="center">15</td><td align="center">16</td><td class="CalWeekendDay" align="center">17</td></tr><tr><td class="CalWeekendDay" align="center">18</td><td align="center">19</td><td align="center">20</td><td align="center">21</td><td align="center">22</td><td class="CalTodayDay" align="center">23</td><td class="CalWeekendDay" align="center">24</td></tr><tr><td class="CalWeekendDay" align="center">25</td><td align="center">26</td><td align="center">27</td><td align="center">28</td><td align="center">29</td><td align="center">30</td><td class="CalWeekendDay" align="center">31</td></tr><tr><td class="CalOtherMonthDay" align="center">1</td><td class="CalOtherMonthDay" align="center">2</td><td class="CalOtherMonthDay" align="center">3</td><td class="CalOtherMonthDay" align="center">4</td><td class="CalOtherMonthDay" align="center">5</td><td class="CalOtherMonthDay" align="center">6</td><td class="CalOtherMonthDay" align="center">7</td></tr>
</tbody></table></div><script type="text/javascript">loadBlogDefaultCalendar();</script><br>
			<div id="blog-sidecolumn"><div id="sidebar_search" class="sidebar-block">
<div id="sidebar_search" class="mySearch">
<h3 class="catListTitle">搜索</h3>
<div id="sidebar_search_box">
<div id="widget_my_zzk" class="div_my_zzk"><input type="text" id="q" onkeydown="return zzk_go_enter(event);" class="input_my_zzk">&nbsp;<input onclick="zzk_go()" type="button" value="找找看" id="btnZzk" class="btn_my_zzk"></div>
<div id="widget_my_google" class="div_my_zzk"><input type="text" name="google_q" id="google_q" onkeydown="return google_go_enter(event)" class="input_my_zzk">&nbsp;<input onclick="google_go()" type="button" value="谷歌搜索" class="btn_my_zzk"></div>
</div>
</div>

</div><div id="sidebar_shortcut" class="sidebar-block">
<h3 class="catListTitle">常用链接</h3>
<ul>
<li><a href="http://www.cnblogs.com/tornadomeet/p/" title="我的博客的随笔列表">我的随笔</a></li><li><a href="http://www.cnblogs.com/tornadomeet/MyComments.html" title="我发表过的评论列表">我的评论</a></li><li><a href="http://www.cnblogs.com/tornadomeet/OtherPosts.html" title="我评论过的随笔列表">我的参与</a></li><li><a href="http://www.cnblogs.com/tornadomeet/RecentComments.html" title="我的博客的评论列表">最新评论</a></li><li><a href="http://www.cnblogs.com/tornadomeet/tag/" title="我的博客的标签列表">我的标签</a></li>
</ul>
<div id="itemListLin_con" style="display:none;">

</div></div><div id="sidebar_toptags" class="sidebar-block">
<h3 class="catListTitle">我的标签</h3>
<div id="MyTag">
<ul>
<li><a href="http://www.cnblogs.com/tornadomeet/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>(72)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/Deep%20Learning/">Deep Learning</a>(51)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/opencv/">opencv</a>(34)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8Bopencv/">基础学习笔记之opencv</a>(24)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/Android%E5%BC%80%E5%8F%91%E5%8E%86%E7%A8%8B/">Android开发历程</a>(18)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/matlab/">matlab</a>(16)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/reading%20papers/">reading papers</a>(16)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/%E6%80%BB%E7%BB%93%E7%B3%BB%E5%88%97/">总结系列</a>(15)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/Qt%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/">Qt学习之路</a>(14)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/OpenNI/">OpenNI</a>(14)</li><li><a href="http://www.cnblogs.com/tornadomeet/tag/">更多</a></li>
</ul>
</div></div><div id="sidebar_categories">
		<h3 class="catListTitle">随笔分类<span style="font-size:11px;font-weight:normal">(468)</span></h3>
		
				<ul class="catList">
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_0" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/400063.html">Android(19)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_1" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/362086.html">ARM</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_2" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/362087.html">C/C++(6)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_3" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361470.html">CV(47)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_4" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/497607.html">Deep Learning(51)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_5" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361469.html">DIP(7)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_6" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/437087.html">Eigen(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_7" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361817.html">FPGA</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_8" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/366103.html">IR(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_9" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/400062.html">Java</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_10" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/416316.html">Kinect(15)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_11" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361818.html">Linux(2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_12" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361467.html">matlab(17)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_13" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361466.html">OpenCV(57)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_14" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/406166.html">OpenGL(7)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_15" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/416317.html">OpenNI(14)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_16" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/379510.html">Paper(8)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_17" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/374732.html">Qt(36)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_18" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361814.html">Robot(2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_19" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/402881.html">XML(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_20" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361815.html">单片机</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_21" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361816.html">电子设计</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_22" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/599067.html">感悟总结(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_23" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361811.html">机器学习(91)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_24" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/371256.html">计算机网络(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_25" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/375969.html">控制理论(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_26" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361812.html">模式识别(11)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_27" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361819.html">嵌入式</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_28" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361813.html">人工智能(8)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_29" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/362088.html">神经网络(2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_30" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/444797.html">手势识别(3)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_31" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/426653.html">数据结构(6)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_32" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/489160.html">数据挖掘(13)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_33" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/370489.html">数学(2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_34" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/362085.html">数字信号处理(1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_35" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/361810.html">算法(7)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_36" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/366102.html">语音处理(4)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_37" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/376567.html">总结(24)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_0_Link_38" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/370949.html">最优化(2)</a></li>
			
				</ul>
			
	
		<h3 class="catListTitle">随笔档案<span style="font-size:11px;font-weight:normal">(252)</span></h3>
		
				<ul class="catList">
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_0" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2014/07.html">2014年7月 (1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_1" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2014/01.html">2014年1月 (6)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_2" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/12.html">2013年12月 (3)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_3" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/11.html">2013年11月 (7)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_4" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/10.html">2013年10月 (1)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_5" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/09.html">2013年9月 (2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_6" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/08.html">2013年8月 (6)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_7" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/07.html">2013年7月 (2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_8" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/06.html">2013年6月 (5)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_9" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/05.html">2013年5月 (8)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_10" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/04.html">2013年4月 (18)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_11" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/03.html">2013年3月 (20)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_12" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/02.html">2013年2月 (2)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_13" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2013/01.html">2013年1月 (4)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_14" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/12.html">2012年12月 (15)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_15" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/11.html">2012年11月 (15)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_16" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/10.html">2012年10月 (8)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_17" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/09.html">2012年9月 (11)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_18" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/08.html">2012年8月 (24)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_19" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/07.html">2012年7月 (29)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_20" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/06.html">2012年6月 (15)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_21" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/05.html">2012年5月 (14)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_22" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/04.html">2012年4月 (14)</a></li>
			
				<li class="catListItem"> <a id="CatList_LinkList_1_Link_23" class="listitem" href="http://www.cnblogs.com/tornadomeet/archive/2012/03.html">2012年3月 (22)</a></li>
			
				</ul>
			
	
		<h3 class="catListTitle">文章分类</h3>
		
				<ul class="catList">
			
				<li class="catListItem"> <a id="CatList_LinkList_2_Link_0" class="listitem" href="http://www.cnblogs.com/tornadomeet/category/599066.html">感悟总结</a></li>
			
				</ul>
			
	
</div><div id="sidebar_scorerank" class="sidebar-block">
<h3>积分与排名</h3>
<ul>
	<li>
		积分 -
		700769
	</li><li>
		排名 -
		120
	</li>
</ul>
</div><div id="sidebar_recentcomments" class="sidebar-block"><div id="recent_comments_wrap">
<h3 class="catListTitle">最新评论</h3>
<div class="RecentComment" id="RecentComments">
	<div id="RecentCommentsBlock"><ul>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html#3578206">1. Re:Deep learning：三十八(Stacked CNN简单介绍)</a></li>
        <li class="recent_comment_body">您好，“ C3层的每个特征图并不一定是都与S2层的特征图相连接，有可能只与其中的某几个连接”，这里的“某几个”是按什么样的规律呢，您又是如何选择的呢，期待您的回复</li>
        <li class="recent_comment_author">--勾勒爱之年华</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2012/05/24/2515980.html#3577217">2. Re:本人常用资源整理(ing...)</a></li>
        <li class="recent_comment_body">好人！ 厉害！</li>
        <li class="recent_comment_author">--ZaneWang</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#3574399">3. Re:Deep learning：四十八(Contractive AutoEncoder简单理解)</a></li>
        <li class="recent_comment_body">博主，紧急求教啊，我主要研究故障诊断方面，但是利用CAE提取特征时发现各种类型的故障提取的特征是一样的，所以我的分类精度只有25%，求指教</li>
        <li class="recent_comment_author">--qiyumeimei</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/16/3024292.html#3573005">4. Re:Deep learning：二十九(Sparse coding练习)</a></li>
        <li class="recent_comment_body">@wanwan0508你好，请问这个问题你拓扑结构下这个check的错误率过大的问题解决了吗？我按照楼主注释掉规则化中偏移patches均值那一步骤后，如下% Rescale from [-1,1] ......</li>
        <li class="recent_comment_author">--三山半</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html#3572155">5. Re:Deep learning：三十八(Stacked CNN简单介绍)</a></li>
        <li class="recent_comment_body">您好，我现在正在仿照DLtoolbox写CNN，工具箱中我采用了您说的“其中打X了的表示两者之间有连接的”，但是在反向BP的时候，该怎么样进行反向BP的传播呢。</li>
        <li class="recent_comment_author">--勾勒爱之年华</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/19/2694332.html#3571293">6. Re:Qt学习之路_12(简易数据管理系统)</a></li>
        <li class="recent_comment_body">求源码 1224373565@qq.com</li>
        <li class="recent_comment_author">--漫步云海</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#3568499">7. Re:Deep learning：四十八(Contractive AutoEncoder简单理解)</a></li>
        <li class="recent_comment_body">@bigiceberg_请问你解决那个求导的问题了吗？是不是用BP算法求残差再求导？...</li>
        <li class="recent_comment_author">--qiyumeimei</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/p/3434651.html#3568261">8. Re:Deep learning：四十八(Contractive AutoEncoder简单理解)</a></li>
        <li class="recent_comment_body">你好，我想问一下那个雅克比矩阵是不是只针对编码网络部分？</li>
        <li class="recent_comment_author">--qiyumeimei</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/22/2975978.html#3560796">9. Re:Deep learning：十三(Softmax Regression)</a></li>
        <li class="recent_comment_body">损失函数中指示函数"1{.}"写错了吧。。。应该为“0{.}”？<br>损失函数不是应该在预测正确时不惩罚，预测错误时惩罚么。。貌似写反了。</li>
        <li class="recent_comment_author">--欧麦高德</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/28/2420936.html#3560745">10. Re:基础学习笔记之opencv(3)：haartraining生成.xml文件过程</a></li>
        <li class="recent_comment_body">博主您好，我在生成.vec文件时，出现了pos.txt(1): parse errorDone. Create 0 samples 请问这是什么问题呢？命令如下：-info pos.txt -vec......</li>
        <li class="recent_comment_author">--zyriris</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/20/2970724.html#3559683">11. Re:Deep learning：九(Sparse Autoencoder练习)</a></li>
        <li class="recent_comment_body">想请教几个问题，谁能加我的qq：9315387，谢谢！另外，最后显示的是权重w，显示w有什么意义？不如显示特征</li>
        <li class="recent_comment_author">--admudzl</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/p/3468450.html#3557512">12. Re:Deep learning：五十一(CNN的反向求导及练习)</a></li>
        <li class="recent_comment_body">楼主你好 ufldl上说在更新参数的时候要在20分钟之内 我的程序跑了40分钟 请问有什么最值得优化的地方 或者说最耗时的地方</li>
        <li class="recent_comment_author">--kimir17</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/19/2970101.html#3557043">13. Re:Deep learning：八(Sparse Autoencoder)</a></li>
        <li class="recent_comment_body">“其中的参数一般取很小，比如说0.05，也就是小概率发生事件的概率。这说明要求隐含层的每一个节点的输出均值接近0.05”----楼主能帮忙再解释下么，多谢~</li>
        <li class="recent_comment_author">--欧麦高德</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/18/2966041.html#3556597">14. Re:Deep learning：七(基础知识_2)</a></li>
        <li class="recent_comment_body">楼主您好。<br>请问“隐含层神经元的个数越多则效果会越好”，这个不绝对吧？</li>
        <li class="recent_comment_author">--欧麦高德</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/26/2704046.html#3554242">15. Re:Kinect+OpenNI学习笔记之1(开发环境的建立)</a></li>
        <li class="recent_comment_body">楼主，你好。我驱动安装成功后的设备管理器处会显示没有kinect motor，请问你上面的说“手动更新驱动程序到指定的安装目录”如何实现。</li>
        <li class="recent_comment_author">--骋</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/28/2568634.html#3554202">16. Re:Qt学习之路_4(Qt UDP的初步使用)</a></li>
        <li class="recent_comment_body">@kyww我也出现过这种情况，不过解决了，你现在解决了吗...</li>
        <li class="recent_comment_author">--彷徨中前行的我</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/14/2959138.html#3553429">17. Re:Deep learning：一(基础知识_1)</a></li>
        <li class="recent_comment_body">您好，请问文中“牛顿法不需要选择任何参数”怎么理解？？多谢~~</li>
        <li class="recent_comment_author">--欧麦高德</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/24/2979408.html#3537905">18. Re:Deep learning：十五(Self-Taught Learning练习)</a></li>
        <li class="recent_comment_body">请问博主，对于这种self taught learning的hiddensize怎么确定？ 我自己用1000个数据试了一下：当hiddensize=200，准确率=90%；当hiddensize = ......</li>
        <li class="recent_comment_author">--zoey321</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/p/3258122.html#3535043">19. Re:Deep learning：四十一(Dropout简单理解)</a></li>
        <li class="recent_comment_body">博文里提到的native bayes，应该是naive bayes（朴素贝叶斯）。看了下论文原文，dropout可以看作是bagging的一个特例，博主这里提到的boosting应该是笔误吧，boos......</li>
        <li class="recent_comment_author">--ChrisZZ</li>
        <li class="recent_comment_title"><a href="http://www.cnblogs.com/tornadomeet/p/3468450.html#3534132">20. Re:Deep learning：五十一(CNN的反向求导及练习)</a></li>
        <li class="recent_comment_body">@tornadomeet您好，我想请问一下，怎么换成自己的数据，我想看看效果，希望能指点，谢谢！...</li>
        <li class="recent_comment_author">--susanwq</li>
</ul>
</div>
</div>
</div></div><div id="sidebar_topviewedposts" class="sidebar-block"><div id="topview_posts_wrap">
<h3 class="catListTitle">阅读排行榜</h3>
<div class="RecentComment" id="TopViewPosts"> 
	<div id="TopViewPostsBlock"><ul><li><a href="http://www.cnblogs.com/tornadomeet/p/3395593.html">1. 机器学习&amp;数据挖掘笔记_16（常见面试之机器学习算法思想简单梳理）(91423)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3468450.html">2. Deep learning：五十一(CNN的反向求导及练习)(66124)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/14/2959138.html">3. Deep learning：一(基础知识_1)(65245)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/27/2984725.html">4. Deep learning：十九(RBM简单理解)(58845)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3439503.html">5. Deep learning：四十九(RNN-RBM简单理解)(57656)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html">6. Deep learning：三十八(Stacked CNN简单介绍)(57324)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3258122.html">7. Deep learning：四十一(Dropout简单理解)(56485)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/22/2975978.html">8. Deep learning：十三(Softmax Regression)(53442)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/12/26/2834336.html">9. 基础学习笔记之opencv(24)：imwrite函数的使用(53208)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/17/2644903.html">10. 特征点检测学习_2(surf算法)(52197)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/15/2640754.html">11. opencv源码解析之(6)：hog源码分析(47938)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/20/2970724.html">12. Deep learning：九(Sparse Autoencoder练习)(41136)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/03/2621814.html">13. 目标检测学习_1(用opencv自带hog实现行人检测)(40436)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/15/2398769.html">14. 目标跟踪学习笔记_1(opencv中meanshift和camshift例子的应用)(39407)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/05/24/2515980.html">15. 本人常用资源整理(ing...)(36185)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/22/2651574.html">16. OpenGL_Qt学习笔记之_01(创建一个OpenGL窗口)(34143)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/06/2538695.html">17. 图像分割学习笔记_1(opencv自带meanshift分割例子)(33827)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/06/2673104.html">18. PCA算法学习_1(OpenCV中PCA实现人脸降维)(33738)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/23/2699077.html">19. Qt学习之路_14(简易音乐播放器)(32798)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3261247.html">20. Deep learning：四十二(Denoise Autoencoder简单理解)(32436)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/22/2411318.html">21. 基础学习笔记之opencv(1)：opencv中facedetect例子浅析(32378)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/19/2970101.html">22. Deep learning：八(Sparse Autoencoder)(31901)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/12/30/2839615.html">23. PCA算法学习_2(PCA理论的matlab实现)(30933)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/28/2568634.html">24. Qt学习之路_4(Qt UDP的初步使用)(29529)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/13/3018393.html">25. Deep learning：二十六(Sparse coding简单理解)(29322)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/16/2643168.html">26. 特征点检测学习_1(sift算法)(29290)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/09/3009830.html">27. Deep learning：二十三(Convolution和Pooling练习)(28130)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/15/2961660.html">28. Deep learning：二(linear regression练习)(28080)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/24/2560261.html">29. 本人部分博客导航(ing...)(27773)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/23/2413363.html">30. HMM学习笔记_1(从一个实例中学习DTW算法)(27299)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/11/06/2756361.html">31. 一些知识点的初步理解_7(随机森林,ing...)(26134)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/19/2694332.html">32. Qt学习之路_12(简易数据管理系统)(26035)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/30/2571001.html">33. Qt学习之路_5(Qt TCP的初步使用)(25377)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/26/2982694.html">34. Deep learning：十八(关于随机采样)(23768)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/28/2420936.html">35. 基础学习笔记之opencv(3)：haartraining生成.xml文件过程(23764)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/24/2415583.html">36. HMM学习笔记_2(从一个实例中学习HMM前向算法)(23630)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/08/2384843.html">37. opencv源码解析之(3)：特征点检查前言1(23283)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/16/2963919.html">38. Deep learning：四(logistic regression练习)(23230)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/11/23/2783709.html">39. 基础学习笔记之opencv(18)：kmeans函数使用实例(22463)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/19/2646412.html">40. 目标跟踪学习笔记_5(opencv中kalman点跟踪例子)(22373)</a></li></ul></div>
</div>
</div></div><div id="sidebar_topcommentedposts" class="sidebar-block"><div id="topfeedback_posts_wrap">
<h3 class="catListTitle">评论排行榜</h3>
<div class="RecentComment" id="TopCommentsPosts">
	<div id="TopFeedbackPostsBlock"><ul><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/20/2970724.html">1. Deep learning：九(Sparse Autoencoder练习)(98)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/09/3011209.html">2. Deep learning：二十四(stacked autoencoder练习)(77)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/30/3052349.html">3. Deep learning：三十五(用NN实现数据降维练习)(73)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/23/2977621.html">4. Deep learning：十四(Softmax Regression练习)(70)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/16/3024292.html">5. Deep learning：二十九(Sparse coding练习)(66)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/11/04/2753185.html">6. Kinect+OpenNI学习笔记之12(简单手势所表示的数字的识别)(57)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/24/2979408.html">7. Deep learning：十五(Self-Taught Learning练习)(55)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/28/2420936.html">8. 基础学习笔记之opencv(3)：haartraining生成.xml文件过程(53)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/09/3009830.html">9. Deep learning：二十三(Convolution和Pooling练习)(53)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html">10. Deep learning：三十八(Stacked CNN简单介绍)(53)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3468450.html">11. Deep learning：五十一(CNN的反向求导及练习)(44)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/24/2415889.html">12. HMM学习笔记_3(从一个实例中学习Viterbi算法)(38)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/08/2384843.html">13. opencv源码解析之(3)：特征点检查前言1(32)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/10/18/2728896.html">14. Kinect+OpenNI学习笔记之8(Robert Walter手部提取代码的分析)(32)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/17/2644903.html">15. 特征点检测学习_2(surf算法)(31)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/22/2411318.html">16. 基础学习笔记之opencv(1)：opencv中facedetect例子浅析(31)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/10/19/2730891.html">17. Kinect+OpenNI学习笔记之9(不需要骨骼跟踪的人体手部分割)(30)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/15/2640754.html">18. opencv源码解析之(6)：hog源码分析(29)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/08/3007435.html">19. Deep learning：二十二(linear decoder练习)(29)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3874378.html">20. 告别学生时代(28)</a></li></ul></div>
</div></div></div><div id="sidebar_topdiggedposts" class="sidebar-block"><div id="topdigg_posts_wrap">
<h3 class="catListTitle">推荐排行榜</h3>
<div class="RecentComment">
	<div id="TopDiggPostsBlock"><ul><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/05/24/2515980.html">1. 本人常用资源整理(ing...)(36)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3395593.html">2. 机器学习&amp;数据挖掘笔记_16（常见面试之机器学习算法思想简单梳理）(25)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/14/2959138.html">3. Deep learning：一(基础知识_1)(18)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/15/2640754.html">4. opencv源码解析之(6)：hog源码分析(16)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html">5. Deep learning：三十八(Stacked CNN简单介绍)(11)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/24/2560261.html">6. 本人部分博客导航(ing...)(11)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/30/3052349.html">7. Deep learning：三十五(用NN实现数据降维练习)(8)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/11/04/2753185.html">8. Kinect+OpenNI学习笔记之12(简单手势所表示的数字的识别)(8)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3258122.html">9. Deep learning：四十一(Dropout简单理解)(7)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/11/12/2766458.html">10. 龙星计划机器学习笔记(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/23/2699077.html">11. Qt学习之路_14(简易音乐播放器)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/22/2698337.html">12. Qt学习之路_13(简易俄罗斯方块)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/19/2694332.html">13. Qt学习之路_12(简易数据管理系统)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/17/2644903.html">14. 特征点检测学习_2(surf算法)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3874378.html">15. 告别学生时代(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3468450.html">16. Deep learning：五十一(CNN的反向求导及练习)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/02/2531565.html">17. 前景检测算法_3(GMM)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/15/2398769.html">18. 目标跟踪学习笔记_1(opencv中meanshift和camshift例子的应用)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/24/2415889.html">19. HMM学习笔记_3(从一个实例中学习Viterbi算法)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/04/12/2443993.html">20. 初步体验libsvm用法1(官方自带工具)(6)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/24/2415583.html">21. HMM学习笔记_2(从一个实例中学习HMM前向算法)(5)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/27/2420088.html">22. 基础学习笔记之opencv(2)：haartraining前将统一图片尺寸方法(5)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/20/2408086.html">23. Matlab DIP(瓦)ch9形态学图像处理(5)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/16/2643168.html">24. 特征点检测学习_1(sift算法)(5)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3276753.html">25. 机器学习&amp;数据挖掘笔记_14（GMM-HMM语音识别简单理解）(5)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/p/3439503.html">26. Deep learning：四十九(RNN-RBM简单理解)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/09/27/2706417.html">27. Kinect+OpenNI学习笔记之2(获取kinect的颜色图像和深度图像)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/11/09/2763271.html">28. 基础学习笔记之opencv(16)：grabcut使用例程(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/12/30/2839615.html">29. PCA算法学习_2(PCA理论的matlab实现)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/15/2961660.html">30. Deep learning：二(linear regression练习)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/12/12/2813939.html">31. 基础学习笔记之opencv(23)：OpenCV坐标体系的初步认识(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/12/04/2800701.html">32. 基础学习笔记之opencv(20)：OpenCV中的颜色空间(ing...)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/03/23/2413363.html">33. HMM学习笔记_1(从一个实例中学习DTW算法)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/03/2621814.html">34. 目标检测学习_1(用opencv自带hog实现行人检测)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/07/31/2616180.html">35. Qt学习之路_8(Qt中与文件目录相关操作)(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/06/15/3137239.html">36. 机器学习&amp;数据挖掘笔记_11（高斯过程回归）(4)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/06/14/3135380.html">37. 机器学习&amp;数据挖掘笔记_10（高斯过程简单理解）(3)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/05/07/3065953.html">38. Deep learning：三十九(ICA模型练习)(3)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2013/04/09/3009830.html">39. Deep learning：二十三(Convolution和Pooling练习)(3)</a></li><li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/07/30/2615913.html">40. Qt学习之路_7(线性布局和网格布局初步探索)(3)</a></li></ul></div>
</div></div></div></div><script type="text/javascript">loadBlogSideColumn();</script></div>
	
</div>

<!--done-->
<div class="footer">
	Powered by: <a href="http://www.cnblogs.com/">博客园</a>	模板提供：<a href="http://blog.hjenglish.com/">沪江博客</a>
	Copyright ©2016 tornadomeet
</div>



<!--PageEndHtml Block Begin-->
阿萨德发斯蒂芬
<!--PageEndHtml Block End-->


</body></html>